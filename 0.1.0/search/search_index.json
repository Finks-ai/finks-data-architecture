{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Finks Data Pipeline Documentation","text":"<p>This site contains the architecture and design documentation for Finks Data.</p>"},{"location":"#sections-to-clarify","title":"Sections to Clarify","text":"<p>During a review of the architecture diagrams, the following points were identified as needing clarification:</p> <ol> <li> <p>Metadata Capture in Data Ingestion Flow: The \"Data Ingestion Flow\" diagram in the Detailed Flow section shows the ECS Task writing to the Bronze S3 bucket, and separately shows the Prefect Trigger writing to a Metadata DynamoDB table. It's unclear what specific metadata is being captured and by which process. Does the ECS task write operational metadata (e.g., file size, record count) while the Prefect trigger writes orchestration metadata (e.g., flow run ID, parameters)? Clarifying this would improve the understanding of data lineage.</p> </li> <li> <p>Quality Gate in Data Processing Flow: The \"Data Processing Flow\" diagram in the Detailed Flow section includes a \"Quality Gate\" after the Glue Job. It is unclear what this gate represents. Is it a separate, dedicated service that validates the data, or is it a step within the Glue job itself? Defining this process more explicitly would clarify the data promotion criteria from Bronze to Silver.</p> </li> </ol>"},{"location":"#environment-overview","title":"Environment Overview","text":"<p>The platform supports three environments: - Development: For active development (<code>dev-*-zone</code> buckets) - Staging: Production-like testing (<code>staging-*-zone</code> buckets) - Production: Live data processing (<code>prod-*-zone</code> buckets)</p> <p>See Environment Management for complete configuration details.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Architecture Overview</li> <li>Core Components</li> <li>Cost Optimization</li> <li>Data Lake Architecture</li> <li>Detailed Flow</li> <li>Infrastructure as Code</li> <li>Monitoring and Observability</li> <li>Repositories</li> <li>Security Architecture</li> <li>Service Architecture</li> <li>Technology Decisions</li> <li>Technology Stack</li> <li>Transformation Layer</li> </ul>"},{"location":"#roadmaps","title":"Roadmaps","text":"<ul> <li>Project Roadmap (3-Month)</li> <li>Aggressive Timeline (6-Week)</li> <li>Agentic Sessions</li> <li>Week 0: Preparation</li> <li>Week 1: Foundation</li> <li>Week 2: Infrastructure</li> <li>Week 3: Orchestration</li> <li>Week 4: Transformation</li> <li>Week 5: Pipeline</li> <li>Week 6: Production</li> <li>Best Practices</li> </ul>"},{"location":"infrastructure-as-code/","title":"details","text":""},{"location":"infrastructure-as-code/#infrastructure-as-code-with-pulumi","title":"Infrastructure as Code with Pulumi","text":""},{"location":"infrastructure-as-code/#why-pulumi","title":"Why Pulumi?","text":"<ul> <li>Real Programming Languages: Use TypeScript, Python, Go, or C# instead of DSLs</li> <li>Type Safety: Catch infrastructure errors at compile time</li> <li>Reusable Components: Create custom component resources</li> <li>State Management: Built-in state management with encryption</li> <li>Multi-Cloud Ready: Same codebase can deploy to AWS, Azure, or GCP</li> </ul>"},{"location":"infrastructure-as-code/#pulumi-architecture-pattern","title":"Pulumi Architecture Pattern","text":"<pre><code>finks-infrastructure/\n\u251c\u2500\u2500 __main__.py                 # Main entry point\n\u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 networking.py          # VPC, subnets, security groups\n\u2502   \u251c\u2500\u2500 data_lake.py           # S3 buckets, lifecycle policies\n\u2502   \u251c\u2500\u2500 orchestration.py       # Prefect infrastructure\n\u2502   \u251c\u2500\u2500 ingestion.py           # ECS task definitions\n\u2502   \u2514\u2500\u2500 transformation.py      # dbt ECS service, Glue jobs\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 dev.yaml              # Development environment config\n\u2502   \u251c\u2500\u2500 staging.yaml          # Staging environment config\n\u2502   \u2514\u2500\u2500 prod.yaml             # Production environment config\n\u2514\u2500\u2500 policies/                  # Pulumi policy packs for compliance\n</code></pre>"},{"location":"infrastructure-as-code/#component-resource-pattern","title":"Component Resource Pattern","text":"<p>Pulumi components encapsulate related infrastructure:</p> <pre><code>class DataLake(pulumi.ComponentResource):\n    def __init__(self, name, args, opts=None):\n        super().__init__('custom:storage:DataLake', name, {}, opts)\n\n        # Bronze zone bucket with versioning and lifecycle\n        self.bronze_zone_bucket = s3.Bucket(...)\n\n        # Silver zone bucket with different retention\n        self.silver_zone_bucket = s3.Bucket(...)\n\n        # Gold zone bucket optimized for queries\n        self.gold_zone_bucket = s3.Bucket(...)\n\n        # Glue database for all zones\n        self.glue_database = glue.Database(...)\n</code></pre>"},{"location":"architecture-overview/","title":"Overview","text":""},{"location":"architecture-overview/#architecture-overview","title":"Architecture Overview","text":"<p>This architecture implements a modern ELT (Extract, Load, Transform) data pipeline for financial data using Prefect for orchestration, AWS native services for compute and storage, Pulumi for infrastructure as code, and dbt-core for transformations.</p>"},{"location":"architecture-overview/#key-principles","title":"Key Principles","text":"<ul> <li>Foundation First: Infrastructure as Code, CI/CD, and security before application development</li> <li>ELT over ETL: Load raw data first, transform in-warehouse for flexibility</li> <li>Centralized Orchestration: Prefect Server manages all workflows</li> <li>Clear Service Boundaries: ECS for containerized services, Lambda for event-driven tasks</li> <li>Ephemeral Compute: On-demand task execution rather than always-on services</li> <li>Developer Experience: Local development environment mirrors production</li> <li>Idempotent by Design: Workflows are designed to be safely re-run. A failed flow that is restarted should produce the same result as a flow that succeeds on the first try. This is achieved through partitioned data writes and atomic operations</li> </ul>"},{"location":"architecture-overview/#high-level-architecture-diagram","title":"High-Level Architecture Diagram","text":"<pre><code>graph TD\n    A[\"Infrastructure Layer&lt;br/&gt;(Pulumi - TypeScript/Python)\"] --&gt; B[\"Orchestration Layer&lt;br/&gt;(Prefect Server on ECS)\"];\n    B --&gt; C[\"Ingestion&lt;br/&gt;Layer\"];\n    B --&gt; D[\"Processing&lt;br/&gt;Layer\"];\n    B --&gt; E[\"Serving&lt;br/&gt;Layer\"];</code></pre>"},{"location":"core-components/","title":"Core Components","text":""},{"location":"core-components/#core-components","title":"Core Components","text":""},{"location":"core-components/#1-infrastructure-foundation","title":"1. Infrastructure Foundation","text":"<ul> <li>Pulumi: Infrastructure as Code in TypeScript/Python</li> <li>AWS VPC: Private network with public/private subnets</li> <li>Secrets Manager: Centralized credential management</li> <li>CI/CD Pipeline: GitHub Actions for automated deployments</li> </ul>"},{"location":"core-components/#2-orchestration-layer","title":"2. Orchestration Layer","text":"<ul> <li>Prefect Server: Hosted on ECS Fargate</li> <li>Prefect Agents: Multiple agent pools for workload isolation</li> <li>PostgreSQL RDS: Metadata storage with multi-AZ deployment</li> <li>Prefect Blocks: Reusable configuration for AWS resources</li> </ul>"},{"location":"core-components/#configuration-as-code-managing-prefect-blocks","title":"Configuration as Code: Managing Prefect Blocks","text":"<p>All Prefect Blocks (for S3 storage, AWS credentials, etc.) are defined and managed as code, typically within the same Pulumi or a separate Python project. They are applied to the Prefect server via a CI/CD script. This ensures that configuration is version-controlled, auditable, and consistent across all environments, eliminating manual UI setup.</p> <pre><code># Example: Defining blocks in a Python script\nfrom prefect_aws import AwsCredentials, S3Bucket, ECSTask\n\ndef create_prefect_blocks():\n    # AWS credentials block\n    aws_creds = AwsCredentials(\n        aws_access_key_id=\"...\",\n        aws_secret_access_key=\"...\"\n    )\n    aws_creds.save(name=\"production-aws-creds\", overwrite=True)\n\n    # S3 storage block\n    s3_block = S3Bucket(\n        bucket_name=\"bronze-zone\",\n        credentials=aws_creds\n    )\n    s3_block.save(name=\"bronze-storage\", overwrite=True)\n\n    # ECS task block for dbt\n    ecs_task = ECSTask(\n        task_definition_arn=\"arn:aws:ecs:ca-central-1:123456789:task-definition/dbt-core\",\n        cluster=\"transformation-cluster\",\n        vpc_id=\"vpc-xyz\",\n        credentials=aws_creds\n    )\n    ecs_task.save(name=\"dbt-ecs-task\", overwrite=True)\n\nif __name__ == \"__main__\":\n    create_prefect_blocks()\n</code></pre>"},{"location":"core-components/#3-ingestion-layer","title":"3. Ingestion Layer","text":"<ul> <li>ECS Tasks: Ephemeral containers for data extraction</li> <li>MongoDB Connector</li> <li>Financial Modeling Prep API Connector</li> <li>News Streaming Service</li> <li>Kinesis Data Streams: Real-time data ingestion</li> <li>Kinesis Firehose: Automatic batching to S3</li> </ul>"},{"location":"core-components/#4-storage-layer-medallion-architecture","title":"4. Storage Layer - Medallion Architecture","text":"<ul> <li>Bronze Zone: Raw, immutable data</li> <li>Silver Zone: Cleansed, validated, standardized data</li> <li>Gold Zone: Business-ready, aggregated data</li> <li>AWS Glue Catalog: Unified metadata repository</li> <li>Lake Formation: Fine-grained access control</li> </ul>"},{"location":"core-components/#5-transformation-layer","title":"5. Transformation Layer","text":"<ul> <li>dbt-core on ECS: Containerized transformation engine</li> <li>AWS Glue Jobs: Bronze to Silver data cleansing</li> <li>Athena: SQL interface for Silver zone queries</li> <li>Redshift Serverless: Optional warehouse for complex analytics</li> </ul>"},{"location":"core-components/#6-serving-layer","title":"6. Serving Layer","text":"<ul> <li>API Gateway + Lambda: REST/GraphQL APIs</li> <li>Athena: Direct SQL access for analysts</li> <li>QuickSight: Business intelligence dashboards</li> </ul>"},{"location":"cost-optimization/","title":"Cost Optimization","text":""},{"location":"cost-optimization/#cost-optimization-strategy","title":"Cost Optimization Strategy","text":""},{"location":"cost-optimization/#compute-optimization","title":"Compute Optimization","text":"<ul> <li>Graviton Processors: 40% better price-performance</li> <li>Spot Instances: For non-critical batch jobs</li> <li>Fargate Spot: For Prefect agents</li> <li>Right-sizing: Regular analysis of resource usage</li> </ul>"},{"location":"cost-optimization/#storage-optimization","title":"Storage Optimization","text":"<ul> <li>S3 Intelligent-Tiering: Automatic tier management</li> <li>Lifecycle Policies:</li> <li>Bronze: Standard \u2192 Standard-IA (30 days) \u2192 Glacier (90 days)</li> <li>Silver: Standard \u2192 Standard-IA (60 days)</li> <li>Gold: Standard (always hot)</li> <li>Compression: Parquet with Snappy compression</li> <li>Partitioning: Reduce data scanned by queries</li> <li>Small File Compaction: Scheduled Glue jobs merge small files into optimal sizes</li> </ul>"},{"location":"cost-optimization/#small-file-compaction-strategy","title":"Small File Compaction Strategy","text":"<pre><code># Prefect flow for periodic file compaction\n@flow(name=\"small-file-compaction\")\ndef compact_silver_zone_files(date: datetime):\n    \"\"\"Merge small Parquet files into larger, optimized files\"\"\"\n\n    glue_job = GlueJob.load(\"silver-compaction-job\")\n\n    # Run compaction for previous day's data\n    glue_job.run(\n        arguments={\n            \"--partition_date\": date.strftime(\"%Y-%m-%d\"),\n            \"--target_file_size\": \"512MB\",\n            \"--source_path\": f\"s3://silver-zone/entity=trades/year={date.year}/month={date.month:02d}/day={date.day:02d}/\",\n            \"--temp_path\": \"s3://temp-zone/compaction/\"\n        }\n    )\n\n    # Verify compaction results\n    validate_compaction_results(date)\n\n# Schedule to run nightly\ncompact_silver_zone_files_deployment = Deployment.build_from_flow(\n    flow=compact_silver_zone_files,\n    name=\"nightly-compaction\",\n    schedule=CronSchedule(cron=\"0 2 * * *\"),  # 2 AM daily\n    parameters={\"date\": \"{{ yesterday_ds }}\"}\n)\n</code></pre>"},{"location":"cost-optimization/#data-transfer-optimization","title":"Data Transfer Optimization","text":"<ul> <li>VPC Endpoints: Avoid NAT Gateway charges</li> <li>S3 Transfer Acceleration: For large uploads</li> <li>CloudFront: For frequently accessed Gold data</li> </ul>"},{"location":"cost-optimization/#reserved-capacity","title":"Reserved Capacity","text":"<ul> <li>RDS Reserved Instances: For Prefect database</li> <li>Compute Savings Plans: For predictable ECS usage</li> </ul>"},{"location":"data-lake-architecture/","title":"Data Lake","text":""},{"location":"data-lake-architecture/#data-lake-architecture","title":"Data Lake Architecture","text":""},{"location":"data-lake-architecture/#medallion-architecture-pattern","title":"Medallion Architecture Pattern","text":"<p>The Bronze \u2192 Silver \u2192 Gold pattern provides clear data quality tiers:</p>"},{"location":"data-lake-architecture/#bronze-zone-raw-data","title":"Bronze Zone (Raw Data)","text":"<ul> <li>Purpose: Preserve raw data exactly as received</li> <li>Format: Original format (JSON, CSV, API responses)</li> <li>Partitioning: By source, ingestion time</li> <li>Retention: Long-term (archive to Glacier after 90 days)</li> <li>Schema: No schema enforcement</li> </ul> <pre><code>graph TD\n    subgraph Bronze Zone\n        direction LR\n        A[\"s3://bronze-zone/\"] --&gt; B[\"source={source_name}/\"];\n        B --&gt; C[\"year={YYYY}/\"];\n        C --&gt; D[\"month={MM}/\"];\n        D --&gt; E[\"day={DD}/\"];\n        E --&gt; F[\"hour={HH}/\"];\n        F --&gt; G[\"run_id={prefect_run_id}/\"];\n        G --&gt; H[\"{timestamp}_{batch_id}.json\"];\n    end</code></pre>"},{"location":"data-lake-architecture/#silver-zone-cleansed-data","title":"Silver Zone (Cleansed Data)","text":"<ul> <li>Purpose: Standardized, deduplicated, validated data</li> <li>Format: Parquet with compression</li> <li>Partitioning: By business entity and time</li> <li>Retention: Medium-term (1-2 years hot storage)</li> <li>Schema: Enforced via Glue Schema Registry</li> </ul> <pre><code>graph TD\n    subgraph Silver Zone\n        direction LR\n        A[\"s3://silver-zone/\"] --&gt; B[\"entity={entity_name}/\"];\n        B --&gt; C[\"year={YYYY}/\"];\n        C --&gt; D[\"month={MM}/\"];\n        D --&gt; E[\"day={DD}/\"];\n        E --&gt; F[\"{entity}_{version}.parquet\"];\n    end</code></pre>"},{"location":"data-lake-architecture/#gold-zone-business-data","title":"Gold Zone (Business Data)","text":"<ul> <li>Purpose: Aggregated, business-ready datasets</li> <li>Format: Parquet optimized for analytics</li> <li>Partitioning: By use case and access patterns</li> <li>Retention: Based on business needs</li> <li>Schema: Documented business glossary</li> </ul> <pre><code>graph TD\n    subgraph Gold Zone\n        direction LR\n        A[\"s3://gold-zone/\"] --&gt; B[\"domain={business_domain}/\"];\n        B --&gt; C[\"dataset={dataset_name}/\"];\n        C --&gt; D[\"version={schema_version}/\"];\n        D --&gt; E[\"year={YYYY}/\"];\n        E --&gt; F[\"month={MM}/\"];\n        F --&gt; G[\"{dataset}_{date}.parquet\"];\n    end</code></pre>"},{"location":"detailed-flow/","title":"Detailed Flow","text":""},{"location":"detailed-flow/#detailed-flow","title":"Detailed Flow","text":""},{"location":"detailed-flow/#data-ingestion-flow","title":"Data Ingestion Flow","text":"<pre><code>graph TD\n    A[\"Prefect&lt;br/&gt;Trigger\"] --&gt; B[\"ECS Task&lt;br/&gt;(Extract)\"];\n    B --&gt; C[\"Bronze&lt;br/&gt;S3\"];\n    C --&gt; D[\"Metadata&lt;br/&gt;DynamoDB\"];\n    A --&gt; D;</code></pre> <ol> <li>Scheduled Trigger: Prefect Server initiates flow based on schedule or event</li> <li>Ephemeral Extraction: Prefect spins up ECS task for specific data source</li> <li>Raw Data Landing: Extracted data written to Bronze zone with metadata</li> <li>Lineage Tracking: Metadata recorded in DynamoDB for audit trail</li> </ol>"},{"location":"detailed-flow/#handling-historical-backfills","title":"Handling Historical Backfills","text":"<p>The architecture is designed to support large-scale historical backfills. Prefect flows are parameterized (e.g., by <code>start_date</code> and <code>end_date</code>). A backfill can be triggered by running a flow with a date range, and Prefect's concurrency capabilities will schedule and manage the parallel execution of hundreds or thousands of historical tasks.</p> <pre><code>@flow(name=\"backfill-ingestion\")\ndef backfill_historical_data(\n    source: str,\n    start_date: datetime,\n    end_date: datetime,\n    parallel_days: int = 30\n):\n    \"\"\"Backfill historical data with parallel execution\"\"\"\n\n    # Generate date ranges for parallel processing\n    date_ranges = generate_date_chunks(start_date, end_date, parallel_days)\n\n    # Submit parallel ingestion tasks\n    futures = []\n    for date_range in date_ranges:\n        future = ingest_data.submit(\n            source=source,\n            start_date=date_range[0],\n            end_date=date_range[1]\n        )\n        futures.append(future)\n\n    # Wait for all tasks to complete\n    results = [future.result() for future in futures]\n\n    # Trigger downstream processing\n    trigger_silver_processing(source, start_date, end_date)\n</code></pre>"},{"location":"detailed-flow/#data-processing-flow","title":"Data Processing Flow","text":"<pre><code>graph TD\n    A[\"Bronze&lt;br/&gt;S3\"] --&gt; B[\"Glue Job&lt;br/&gt;(Cleanse)\"];\n    B --&gt; C[\"Silver&lt;br/&gt;S3\"];\n    B --&gt; D[\"Quality&lt;br/&gt;Gate\"];</code></pre> <ol> <li>Bronze to Silver: AWS Glue job performs schema validation and cleansing</li> <li>Quality Gates: Data quality checks must pass before promotion</li> <li>Schema Registry: Validated schemas registered for downstream use</li> </ol>"},{"location":"detailed-flow/#transformation-flow","title":"Transformation Flow","text":"<pre><code>graph TD\n    A[\"Silver&lt;br/&gt;S3\"] --&gt; B[\"dbt Core&lt;br/&gt;on ECS\"];\n    B --&gt; C[\"Gold&lt;br/&gt;S3\"];\n    B --&gt; D[\"dbt Tests&lt;br/&gt;&amp; Docs\"];</code></pre> <ol> <li>Business Logic: dbt models transform Silver data to Gold</li> <li>Testing: dbt tests ensure transformation quality</li> <li>Documentation: Auto-generated documentation for lineage</li> </ol>"},{"location":"environment-management/","title":"Environment Management","text":"<p>This document describes how the Finks data pipeline manages multiple environments (development, staging, production) across all repositories and infrastructure components.</p>"},{"location":"environment-management/#overview","title":"Overview","text":"<p>The Finks data platform implements a three-tier environment strategy that provides isolation, safety, and progressive deployment capabilities:</p> <ul> <li>Development (dev): For active development and testing</li> <li>Staging: Production-like environment for integration testing</li> <li>Production (prod): Live data processing environment</li> </ul>"},{"location":"environment-management/#environment-vs-medallion-architecture","title":"Environment vs. Medallion Architecture","text":"<p>It's important to understand that environments and the medallion architecture serve different purposes:</p> <pre><code>graph TB\n    subgraph \"Production Environment\"\n        P1[prod-bronze-zone] --&gt; P2[prod-silver-zone] --&gt; P3[prod-gold-zone]\n    end\n\n    subgraph \"Staging Environment\"  \n        S1[staging-bronze-zone] --&gt; S2[staging-silver-zone] --&gt; S3[staging-gold-zone]\n    end\n\n    subgraph \"Development Environment\"\n        D1[dev-bronze-zone] --&gt; D2[dev-silver-zone] --&gt; D3[dev-gold-zone]\n    end</code></pre> <ul> <li>Environments: Provide isolation between development work and production systems</li> <li>Medallion Architecture: Ensures data quality progression within each environment</li> </ul>"},{"location":"environment-management/#environment-configuration-strategy","title":"Environment Configuration Strategy","text":""},{"location":"environment-management/#configuration-files","title":"Configuration Files","text":"<p>All repositories use Python-dotenv with environment-specific <code>.env</code> files:</p> <pre><code>finks-{repository}/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 .env.dev          # Development configuration\n\u2502   \u251c\u2500\u2500 .env.staging      # Staging configuration  \n\u2502   \u251c\u2500\u2500 .env.prod         # Production configuration\n\u2502   \u251c\u2500\u2500 .env.example      # Template (committed to git)\n\u2502   \u2514\u2500\u2500 config.py         # Configuration loader\n</code></pre>"},{"location":"environment-management/#configuration-loader-pattern","title":"Configuration Loader Pattern","text":"<p>Each repository implements a standard configuration loader:</p> <pre><code># config/config.py\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\ndef load_environment():\n    \"\"\"Load environment-specific .env file\"\"\"\n    env = os.getenv(\"ENVIRONMENT\", \"dev\")\n    env_file = Path(__file__).parent / f\".env.{env}\"\n\n    if not env_file.exists():\n        raise FileNotFoundError(f\"Missing {env_file}\")\n\n    load_dotenv(env_file, override=True)\n    return env\n\nENVIRONMENT = load_environment()\n\nclass Config:\n    # AWS Configuration\n    AWS_REGION = os.getenv(\"AWS_REGION\", \"ca-central-1\")\n    S3_BUCKET_PREFIX = os.getenv(\"S3_BUCKET_PREFIX\")\n\n    # Computed values\n    BRONZE_BUCKET = f\"{S3_BUCKET_PREFIX}-bronze-zone\"\n    SILVER_BUCKET = f\"{S3_BUCKET_PREFIX}-silver-zone\"\n    GOLD_BUCKET = f\"{S3_BUCKET_PREFIX}-gold-zone\"\n</code></pre>"},{"location":"environment-management/#repository-specific-implementation","title":"Repository-Specific Implementation","text":""},{"location":"environment-management/#finks-infrastructure","title":"finks-infrastructure","text":"<p>Uses Pulumi stacks for complete infrastructure isolation:</p> <pre><code>pulumi stack select dev      # Development infrastructure\npulumi stack select staging  # Staging infrastructure\npulumi stack select prod     # Production infrastructure\n</code></pre> <p>Configuration files: <pre><code># config/dev.yaml\naws:region: ca-central-1\ninstance_type: t3.small\nretention_days: 7\n\n# config/prod.yaml\naws:region: ca-central-1\ninstance_type: t3.large\nretention_days: 365\n</code></pre></p>"},{"location":"environment-management/#finks-pipelines","title":"finks-pipelines","text":"<p>Prefect flows are environment-aware:</p> <pre><code># flows/config.py\nfrom config import Config\n\n@flow(name=f\"ingest-fmp-{Config.ENVIRONMENT}\")\ndef ingest_fmp_data():\n    \"\"\"Environment-specific flow\"\"\"\n    target_bucket = Config.BRONZE_BUCKET\n    # Flow implementation\n</code></pre>"},{"location":"environment-management/#finks-dbt","title":"finks-dbt","text":"<p>dbt profiles support multiple environments:</p> <pre><code># profiles.yml\nconfig:\n  send_anonymous_usage_stats: false\n\nfinks:\n  outputs:\n    dev:\n      type: athena\n      region_name: ca-central-1\n      database: dev_analytics\n      s3_staging_dir: s3://dev-athena-results/\n\n    staging:\n      type: athena\n      region_name: ca-central-1\n      database: staging_analytics\n      s3_staging_dir: s3://staging-athena-results/\n\n    prod:\n      type: athena\n      region_name: ca-central-1\n      database: prod_analytics\n      s3_staging_dir: s3://prod-athena-results/\n\n  target: \"{{ env_var('DBT_TARGET', 'dev') }}\"\n</code></pre>"},{"location":"environment-management/#finks-ingestion","title":"finks-ingestion","text":"<p>Connectors read environment-specific targets:</p> <pre><code># connectors/fmp/main.py\nfrom config import Config\n\ndef extract_data():\n    \"\"\"Extract data to environment-specific bucket\"\"\"\n    s3_client = boto3.client('s3')\n    bucket = Config.BRONZE_BUCKET\n    # Extraction logic\n</code></pre>"},{"location":"environment-management/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"environment-management/#progressive-deployment-strategy","title":"Progressive Deployment Strategy","text":"<p>All repositories follow a consistent deployment pattern:</p> <pre><code># .github/workflows/deploy.yml\nname: Deploy\n\non:\n  push:\n    branches: [main]\n  release:\n    types: [published]\n\njobs:\n  deploy-dev:\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Deploy to Development\n        env:\n          ENVIRONMENT: dev\n        run: ./scripts/deploy.sh\n\n  deploy-staging:\n    needs: deploy-dev\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Deploy to Staging\n        env:\n          ENVIRONMENT: staging\n        run: ./scripts/deploy.sh\n\n  deploy-prod:\n    if: github.event_name == 'release'\n    runs-on: ubuntu-latest\n    environment: production  # Requires manual approval\n    steps:\n      - uses: actions/checkout@v3\n      - name: Deploy to Production\n        env:\n          ENVIRONMENT: prod\n        run: ./scripts/deploy.sh\n</code></pre>"},{"location":"environment-management/#environment-specific-resources","title":"Environment-Specific Resources","text":""},{"location":"environment-management/#infrastructure-sizing","title":"Infrastructure Sizing","text":"Resource Development Staging Production ECS Fargate 0.5 vCPU, 1GB 1 vCPU, 2GB 2 vCPU, 4GB RDS PostgreSQL db.t3.micro db.t3.small db.t3.medium (Multi-AZ) S3 Retention 7 days 30 days 365 days Prefect Agents 1 2 3"},{"location":"environment-management/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Dev: Uses Fargate Spot, minimal retention, single AZ</li> <li>Staging: Production-like but single AZ, shorter retention</li> <li>Production: Full HA, multi-AZ, long-term retention</li> </ul>"},{"location":"environment-management/#local-development","title":"Local Development","text":"<p>Developers can run a complete local environment:</p> <pre><code># docker-compose.yml\nservices:\n  prefect-server:\n    image: prefecthq/prefect:latest\n    environment:\n      - PREFECT_SERVER_API_HOST=0.0.0.0\n    ports:\n      - \"4200:4200\"\n\n  localstack:\n    image: localstack/localstack:latest\n    environment:\n      - SERVICES=s3,glue,athena\n    ports:\n      - \"4566:4566\"\n</code></pre> <p>Set environment for local development: <pre><code>export ENVIRONMENT=dev\npython -m flows.main\n</code></pre></p>"},{"location":"environment-management/#security-considerations","title":"Security Considerations","text":""},{"location":"environment-management/#secret-management","title":"Secret Management","text":"<p>Each environment has isolated secrets:</p> <pre><code>AWS Secrets Manager:\n\u251c\u2500\u2500 /dev/prefect/api_key\n\u251c\u2500\u2500 /dev/fmp/api_key\n\u251c\u2500\u2500 /staging/prefect/api_key\n\u251c\u2500\u2500 /staging/fmp/api_key\n\u251c\u2500\u2500 /prod/prefect/api_key\n\u2514\u2500\u2500 /prod/fmp/api_key\n</code></pre>"},{"location":"environment-management/#access-control","title":"Access Control","text":"<ul> <li>Dev: Developers have broad access</li> <li>Staging: Limited to CI/CD and senior developers</li> <li>Production: Restricted access, audit logging enabled</li> </ul>"},{"location":"environment-management/#best-practices","title":"Best Practices","text":""},{"location":"environment-management/#1-environment-promotion","title":"1. Environment Promotion","text":"<p>Always promote code through environments: <pre><code>Local \u2192 Dev \u2192 Staging \u2192 Production\n</code></pre></p>"},{"location":"environment-management/#2-configuration-validation","title":"2. Configuration Validation","text":"<pre><code>def validate_config():\n    \"\"\"Validate configuration based on environment\"\"\"\n    if Config.ENVIRONMENT == \"prod\":\n        assert Config.S3_BUCKET_PREFIX == \"prod\"\n        assert not Config.DEBUG_MODE\n        assert Config.LOG_LEVEL == \"INFO\"\n</code></pre>"},{"location":"environment-management/#3-environment-indicators","title":"3. Environment Indicators","text":"<p>Make it obvious which environment you're in:</p> <pre><code># Prefect flow names\nflow_name = f\"process-data-{Config.ENVIRONMENT}\"\n\n# Log messages\nlogger.info(f\"Starting pipeline in {Config.ENVIRONMENT} environment\")\n\n# Slack notifications\nslack_message = f\"[{Config.ENVIRONMENT.upper()}] Pipeline completed\"\n</code></pre>"},{"location":"environment-management/#4-data-isolation","title":"4. Data Isolation","text":"<p>Never allow cross-environment data access:</p> <pre><code># \u274c Bad\nbucket = \"prod-bronze-zone\"  # Hardcoded\n\n# \u2705 Good  \nbucket = Config.BRONZE_BUCKET  # Environment-aware\n</code></pre>"},{"location":"environment-management/#5-testing-strategy","title":"5. Testing Strategy","text":"<ul> <li>Dev: Unit tests, integration tests with small data</li> <li>Staging: Full integration tests, performance tests</li> <li>Production: Smoke tests only, monitoring</li> </ul>"},{"location":"environment-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"environment-management/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Wrong Environment Loaded <pre><code># Check current environment\necho $ENVIRONMENT\n\n# Force specific environment\nENVIRONMENT=staging python main.py\n</code></pre></p> </li> <li> <p>Missing Environment File <pre><code># Copy from example\ncp config/.env.example config/.env.dev\n# Edit with your values\n</code></pre></p> </li> <li> <p>Cross-Environment Contamination</p> </li> <li>Check S3 bucket names in logs</li> <li>Verify Prefect API URL</li> <li>Confirm database connections</li> </ol>"},{"location":"environment-management/#migration-guide","title":"Migration Guide","text":"<p>When adding environment support to existing code:</p> <ol> <li>Create config directory structure</li> <li>Add <code>.env.*</code> files to <code>.gitignore</code></li> <li>Replace hardcoded values with <code>Config</code> references</li> <li>Update CI/CD workflows</li> <li>Test each environment thoroughly</li> </ol>"},{"location":"environment-management/#summary","title":"Summary","text":"<p>The three-tier environment strategy provides:</p> <ul> <li>Safety: Isolated development and testing</li> <li>Confidence: Production-like staging validation</li> <li>Flexibility: Environment-specific optimizations</li> <li>Cost Control: Right-sized resources per environment</li> </ul> <p>All repositories follow consistent patterns for configuration, deployment, and operation across environments.</p>"},{"location":"infrastructure-as-code/","title":"overview","text":""},{"location":"infrastructure-as-code/#infrastructure-as-code-with-pulumi","title":"Infrastructure as Code with Pulumi","text":""},{"location":"infrastructure-as-code/#why-pulumi","title":"Why Pulumi?","text":"<ul> <li>Real Programming Languages: Use TypeScript, Python, Go, or C# instead of DSLs</li> <li>Type Safety: Catch infrastructure errors at compile time</li> <li>Reusable Components: Create custom component resources</li> <li>State Management: Built-in state management with encryption</li> <li>Multi-Cloud Ready: Same codebase can deploy to AWS, Azure, or GCP</li> </ul>"},{"location":"infrastructure-as-code/#pulumi-architecture-pattern","title":"Pulumi Architecture Pattern","text":"<pre><code>finks-infrastructure/\n\u251c\u2500\u2500 __main__.py                 # Main entry point\n\u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 networking.py          # VPC, subnets, security groups\n\u2502   \u251c\u2500\u2500 data_lake.py           # S3 buckets, lifecycle policies\n\u2502   \u251c\u2500\u2500 orchestration.py       # Prefect infrastructure\n\u2502   \u251c\u2500\u2500 ingestion.py           # ECS task definitions\n\u2502   \u2514\u2500\u2500 transformation.py      # dbt ECS service, Glue jobs\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 dev.yaml              # Development environment config\n\u2502   \u251c\u2500\u2500 staging.yaml          # Staging environment config\n\u2502   \u2514\u2500\u2500 prod.yaml             # Production environment config\n\u2514\u2500\u2500 policies/                  # Pulumi policy packs for compliance\n</code></pre>"},{"location":"infrastructure-as-code/#component-resource-pattern","title":"Component Resource Pattern","text":"<p>Pulumi components encapsulate related infrastructure:</p> <pre><code>class DataLake(pulumi.ComponentResource):\n    def __init__(self, name, args, opts=None):\n        super().__init__('custom:storage:DataLake', name, {}, opts)\n\n        # Bronze zone bucket with versioning and lifecycle\n        self.bronze_zone_bucket = s3.Bucket(...)\n\n        # Silver zone bucket with different retention\n        self.silver_zone_bucket = s3.Bucket(...)\n\n        # Gold zone bucket optimized for queries\n        self.gold_zone_bucket = s3.Bucket(...)\n\n        # Glue database for all zones\n        self.glue_database = glue.Database(...)\n</code></pre>"},{"location":"infrastructure-as-code/#multi-environment-support","title":"Multi-Environment Support","text":"<p>Infrastructure is deployed to separate environments using Pulumi stacks:</p> <pre><code>pulumi stack select dev      # Development environment\npulumi stack select staging  # Staging environment\npulumi stack select prod     # Production environment\n</code></pre> <p>Each environment has its own configuration in <code>config/{env}.yaml</code>. See Environment Management for details.</p>"},{"location":"monitoring-observability/","title":"Monitoring & Observability","text":""},{"location":"monitoring-observability/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"monitoring-observability/#metrics-collection","title":"Metrics Collection","text":"<pre><code>Application Metrics \u2192 CloudWatch \u2192 Grafana\n     \u2502                    \u2502           \u2502\n     \u2514\u2500\u2500 StatsD \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n                                     \u2502\nInfrastructure Metrics \u2192 CloudWatch \u2500\u2500\u2518\n</code></pre>"},{"location":"monitoring-observability/#key-metrics","title":"Key Metrics","text":""},{"location":"monitoring-observability/#pipeline-health","title":"Pipeline Health","text":"<ul> <li>Flow Success Rate: Percentage of successful runs</li> <li>Data Freshness: Time since last successful update</li> <li>Processing Time: End-to-end latency per source</li> <li>Error Rate: Failures by type and stage</li> </ul>"},{"location":"monitoring-observability/#data-quality","title":"Data Quality","text":"<ul> <li>Schema Drift: Changes detected in sources</li> <li>Quality Score: Percentage passing dbt tests</li> <li>Completeness: Records processed vs expected</li> <li>Anomaly Detection: Statistical outliers</li> </ul>"},{"location":"monitoring-observability/#cost-metrics","title":"Cost Metrics","text":"<ul> <li>Cost per GB: Processing cost by data source</li> <li>Resource Utilization: CPU/memory efficiency</li> <li>Storage Growth: Month-over-month trends</li> <li>Query Costs: Athena scan costs by user</li> </ul>"},{"location":"monitoring-observability/#alerting-strategy","title":"Alerting Strategy","text":"<pre><code>Priority Levels:\n\u251c\u2500\u2500 P1: Pipeline Failure (PagerDuty)\n\u251c\u2500\u2500 P2: Quality Degradation (Slack)\n\u251c\u2500\u2500 P3: Cost Anomaly (Email)\n\u2514\u2500\u2500 P4: Performance Warning (Dashboard)\n</code></pre>"},{"location":"monitoring-observability/#observability-stack","title":"Observability Stack","text":"<ul> <li>Logs: CloudWatch Logs \u2192 ElasticSearch</li> <li>Traces: AWS X-Ray for distributed tracing</li> <li>Metrics: CloudWatch + Prometheus</li> <li>Dashboards: Grafana for unified view</li> </ul>"},{"location":"monitoring-observability/#correlated-logs-and-traces","title":"Correlated Logs and Traces","text":"<p>To enable distributed tracing, the Prefect Flow Run ID is injected as an environment variable into every ECS task (Ingestion, Glue, dbt). All logging within the containers is configured to include this ID. This allows for filtering logs in CloudWatch or ElasticSearch to see the complete, end-to-end trace of a single data pipeline execution across all its distributed components.</p> <pre><code># Example: Injecting flow run ID into ECS tasks\n@task\ndef run_ecs_ingestion(source: str):\n    flow_run_id = prefect.context.get(\"flow_run_id\")\n\n    ecs_task = ECSTask.load(\"ingestion-task\")\n    return ecs_task.run(\n        overrides={\n            \"containerOverrides\": [{\n                \"environment\": [\n                    {\"name\": \"PREFECT_FLOW_RUN_ID\", \"value\": flow_run_id},\n                    {\"name\": \"LOG_CORRELATION_ID\", \"value\": flow_run_id},\n                    {\"name\": \"SOURCE\", \"value\": source}\n                ]\n            }]\n        }\n    )\n\n# Container logging configuration\nimport logging\nimport os\n\ncorrelation_id = os.getenv(\"LOG_CORRELATION_ID\", \"unknown\")\nlogging.basicConfig(\n    format=f'%(asctime)s - %(name)s - %(levelname)s - [flow_run_id={correlation_id}] - %(message)s'\n)\n</code></pre>"},{"location":"repositories/","title":"Repository Structure","text":"<p>The Finks data pipeline is organized across multiple repositories, each with a specific purpose and ownership. This separation enables independent development cycles, focused CI/CD pipelines, and clear ownership boundaries.</p>"},{"location":"repositories/#repository-overview","title":"Repository Overview","text":"<pre><code>graph TD\n    A[finks-infrastructure] --&gt;|Deploys| B[AWS Resources]\n    C[finks-pipelines] --&gt;|Orchestrates| D[Data Flows]\n    E[finks-dbt] --&gt;|Transforms| F[Data Models]\n    G[finks-ingestion] --&gt;|Extracts| H[Data Sources]\n\n    C --&gt;|Uses| B\n    E --&gt;|Uses| B\n    G --&gt;|Uses| B\n\n    C --&gt;|Triggers| E\n    C --&gt;|Triggers| G</code></pre>"},{"location":"repositories/#repository-details","title":"Repository Details","text":""},{"location":"repositories/#1-finks-infrastructure","title":"1. finks-infrastructure","text":"<p>Purpose: Infrastructure as Code for all AWS resources</p> <p>Repository: <code>https://github.com/finks-ai/finks-infrastructure</code></p> <p>Owner: Platform/DevOps Engineer</p> <p>Technology Stack: - Pulumi (Python) - AWS SDK - GitHub Actions</p> <p>Contents: <pre><code>finks-infrastructure/\n\u251c\u2500\u2500 __main__.py                 # Main Pulumi entry point\n\u251c\u2500\u2500 components/\n\u2502   \u251c\u2500\u2500 networking.py          # VPC, subnets, security groups\n\u2502   \u251c\u2500\u2500 data_lake.py           # S3 buckets, lifecycle policies\n\u2502   \u251c\u2500\u2500 orchestration.py       # ECS cluster, Prefect infrastructure\n\u2502   \u251c\u2500\u2500 transformation.py      # Glue jobs, Athena setup\n\u2502   \u251c\u2500\u2500 api.py                # API Gateway, Lambda functions\n\u2502   \u2514\u2500\u2500 monitoring.py          # CloudWatch dashboards, alarms\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 dev.yaml              # Development environment config\n\u2502   \u251c\u2500\u2500 staging.yaml          # Staging environment config\n\u2502   \u2514\u2500\u2500 prod.yaml             # Production environment config\n\u251c\u2500\u2500 policies/                  # Pulumi policy packs\n\u251c\u2500\u2500 tests/                    # Infrastructure unit tests\n\u2514\u2500\u2500 .github/\n    \u2514\u2500\u2500 workflows/\n        \u251c\u2500\u2500 preview.yml       # PR preview workflow\n        \u2514\u2500\u2500 deploy.yml        # Deployment workflow\n</code></pre></p> <p>Key Responsibilities: - VPC and networking setup - ECS clusters and task definitions - RDS database provisioning - S3 bucket creation and policies - IAM roles and policies - API Gateway and Lambda functions - CloudWatch resources - Secrets Manager configuration</p> <p>CI/CD Pipeline: 1. Pull Request: <code>pulumi preview</code> shows infrastructure changes 2. Merge to main: Deploy to staging environment 3. Tag release: Deploy to production (with approval)</p>"},{"location":"repositories/#2-finks-pipelines","title":"2. finks-pipelines","text":"<p>Purpose: Prefect flows and orchestration logic</p> <p>Repository: <code>https://github.com/finks-ai/finks-pipelines</code></p> <p>Owner: Data Engineer</p> <p>Technology Stack: - Python 3.12+ (managed with uv) - Prefect 2.x - AWS SDK (boto3) - PySpark (for Glue jobs)</p> <p>Contents: <pre><code>finks-pipelines/\n\u251c\u2500\u2500 flows/\n\u2502   \u251c\u2500\u2500 ingestion/\n\u2502   \u2502   \u251c\u2500\u2500 fmp_api.py        # FMP data ingestion flow\n\u2502   \u2502   \u251c\u2500\u2500 mongodb.py        # MongoDB ingestion flow\n\u2502   \u2502   \u2514\u2500\u2500 streaming.py      # Streaming data flow\n\u2502   \u251c\u2500\u2500 transformation/\n\u2502   \u2502   \u251c\u2500\u2500 bronze_to_silver.py  # Glue job orchestration\n\u2502   \u2502   \u2514\u2500\u2500 silver_to_gold.py    # dbt orchestration\n\u2502   \u2514\u2500\u2500 master_flow.py        # Main orchestration flow\n\u251c\u2500\u2500 blocks/\n\u2502   \u2514\u2500\u2500 configure_blocks.py   # Prefect blocks as code\n\u251c\u2500\u2500 tasks/\n\u2502   \u251c\u2500\u2500 ecs_tasks.py         # ECS task utilities\n\u2502   \u251c\u2500\u2500 glue_tasks.py        # Glue job utilities\n\u2502   \u2514\u2500\u2500 notifications.py     # Alert utilities\n\u251c\u2500\u2500 glue_scripts/\n\u2502   \u251c\u2500\u2500 cleanse_market_data.py\n\u2502   \u2514\u2500\u2500 validate_schemas.py\n\u251c\u2500\u2500 tests/\n\u2514\u2500\u2500 .github/\n    \u2514\u2500\u2500 workflows/\n        \u251c\u2500\u2500 test.yml         # Unit test workflow\n        \u2514\u2500\u2500 deploy_flows.yml # Deploy flows to Prefect\n</code></pre></p> <p>Key Responsibilities: - Define and manage Prefect flows - Orchestrate ECS tasks for ingestion - Trigger AWS Glue jobs - Coordinate dbt transformations - Handle error recovery and notifications - Manage Prefect blocks configuration</p> <p>CI/CD Pipeline: 1. Pull Request: Run unit tests and flow validation 2. Merge to main: Deploy flows to Prefect staging 3. Tag release: Deploy to Prefect production</p>"},{"location":"repositories/#3-finks-dbt","title":"3. finks-dbt","text":"<p>Purpose: Data transformation models and business logic</p> <p>Repository: <code>https://github.com/finks-ai/finks-dbt</code></p> <p>Owner: Analytics Engineer</p> <p>Technology Stack: - dbt-core 1.7+ - dbt-athena adapter - Docker - GitHub Actions</p> <p>Contents: <pre><code>finks-dbt/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 staging/              # Bronze \u2192 Silver models\n\u2502   \u2502   \u251c\u2500\u2500 fmp/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 stg_fmp_quotes.sql\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 stg_fmp_financials.sql\n\u2502   \u2502   \u2514\u2500\u2500 _sources.yml\n\u2502   \u251c\u2500\u2500 intermediate/         # Complex joins and logic\n\u2502   \u2502   \u2514\u2500\u2500 int_market_metrics.sql\n\u2502   \u251c\u2500\u2500 marts/               # Business-ready datasets\n\u2502   \u2502   \u251c\u2500\u2500 finance/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 daily_portfolio_summary.sql\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 monthly_performance.sql\n\u2502   \u2502   \u2514\u2500\u2500 risk/\n\u2502   \u2502       \u2514\u2500\u2500 var_calculations.sql\n\u2502   \u2514\u2500\u2500 _schema.yml          # Model documentation\n\u251c\u2500\u2500 tests/                   # Custom data tests\n\u251c\u2500\u2500 macros/                  # Reusable SQL functions\n\u251c\u2500\u2500 snapshots/              # SCD Type 2 history\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 profiles.yml            # Connection profiles\n\u251c\u2500\u2500 Dockerfile              # Container for ECS\n\u2514\u2500\u2500 .github/\n    \u2514\u2500\u2500 workflows/\n        \u251c\u2500\u2500 slim_ci.yml     # Test changed models only\n        \u2514\u2500\u2500 build.yml       # Build and push Docker image\n</code></pre></p> <p>Key Responsibilities: - Define staging models from Silver zone - Create intermediate transformations - Build business-ready mart models - Implement data quality tests - Document data lineage - Maintain business logic</p> <p>CI/CD Pipeline: 1. Pull Request: dbt slim CI (test changed models) 2. Merge to main: Build Docker image, deploy to staging 3. Tag release: Deploy to production ECR</p>"},{"location":"repositories/#4-finks-ingestion","title":"4. finks-ingestion","text":"<p>Purpose: Containerized data source connectors</p> <p>Repository: <code>https://github.com/finks-ai/finks-ingestion</code></p> <p>Owner: Data Engineer</p> <p>Technology Stack: - Python 3.12+ (managed with uv) - Docker - Various API SDKs - GitHub Actions</p> <p>Contents: <pre><code>finks-ingestion/\n\u251c\u2500\u2500 connectors/\n\u2502   \u251c\u2500\u2500 fmp/\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2502   \u251c\u2500\u2500 main.py          # FMP API connector\n\u2502   \u2502   \u251c\u2500\u2500 schemas.py       # Data validation\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml    # Dependencies managed with uv\n\u2502   \u251c\u2500\u2500 mongodb/\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u2502   \u251c\u2500\u2500 main.py          # MongoDB connector\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml    # Dependencies managed with uv\n\u2502   \u2514\u2500\u2500 news_stream/\n\u2502       \u251c\u2500\u2500 Dockerfile\n\u2502       \u251c\u2500\u2500 main.py          # News streaming connector\n\u2502       \u2514\u2500\u2500 pyproject.toml    # Dependencies managed with uv\n\u251c\u2500\u2500 shared/\n\u2502   \u251c\u2500\u2500 utils.py            # Common utilities\n\u2502   \u251c\u2500\u2500 s3_writer.py        # S3 upload logic\n\u2502   \u2514\u2500\u2500 error_handling.py   # Retry logic\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/\n\u2502   \u2514\u2500\u2500 integration/\n\u2514\u2500\u2500 .github/\n    \u2514\u2500\u2500 workflows/\n        \u251c\u2500\u2500 test.yml        # Test all connectors\n        \u2514\u2500\u2500 build.yml       # Build and push to ECR\n</code></pre></p> <p>Key Responsibilities: - Extract data from various sources - Validate data against schemas - Handle API rate limits and retries - Write raw data to Bronze S3 - Emit metrics and logs - Manage credentials securely</p> <p>CI/CD Pipeline: 1. Pull Request: Run unit and integration tests 2. Merge to main: Build all containers, push to staging ECR 3. Tag release: Push to production ECR</p>"},{"location":"repositories/#cross-repository-dependencies","title":"Cross-Repository Dependencies","text":""},{"location":"repositories/#build-order","title":"Build Order","text":"<ol> <li>finks-infrastructure - Must be deployed first</li> <li>finks-pipelines - Depends on infrastructure</li> <li>finks-dbt - Can be developed in parallel</li> <li>finks-ingestion - Can be developed in parallel</li> </ol>"},{"location":"repositories/#integration-points","title":"Integration Points","text":"<pre><code>graph LR\n    A[Infrastructure] --&gt;|ECS Task Definitions| B[Pipelines]\n    A --&gt;|S3 Buckets| C[dbt]\n    A --&gt;|S3 Buckets| D[Ingestion]\n    B --&gt;|Triggers| C\n    B --&gt;|Triggers| D\n    A --&gt;|ECR Repositories| C\n    A --&gt;|ECR Repositories| D</code></pre>"},{"location":"repositories/#shared-configuration","title":"Shared Configuration","text":"<ul> <li>AWS Region: <code>ca-central-1</code> (defined in infrastructure)</li> <li>S3 Paths: Standardized across all repos</li> <li>IAM Roles: Created by infrastructure, used by others</li> <li>ECR Repositories: Created by infrastructure, images pushed by others</li> </ul>"},{"location":"repositories/#development-workflow","title":"Development Workflow","text":""},{"location":"repositories/#local-development","title":"Local Development","text":"<p>Each repository includes: - Docker Compose for local testing - LocalStack configuration where applicable - Comprehensive README with setup instructions - Pre-commit hooks for code quality</p>"},{"location":"repositories/#branching-strategy","title":"Branching Strategy","text":"<p>All repositories follow GitFlow: - <code>main</code>: Production-ready code - <code>develop</code>: Integration branch - <code>feature/*</code>: Feature development - <code>hotfix/*</code>: Emergency fixes</p>"},{"location":"repositories/#release-process","title":"Release Process","text":"<ol> <li>Create release branch from develop</li> <li>Test in staging environment</li> <li>Create GitHub release with tag</li> <li>Automated deployment to production</li> </ol>"},{"location":"repositories/#security-considerations","title":"Security Considerations","text":""},{"location":"repositories/#access-control","title":"Access Control","text":"<ul> <li>Repository Access: Team-based permissions in GitHub</li> <li>AWS Access: Managed through IAM roles per repository</li> <li>Secrets: Never committed, always use AWS Secrets Manager</li> </ul>"},{"location":"repositories/#code-scanning","title":"Code Scanning","text":"<ul> <li>GitHub Advanced Security enabled</li> <li>Dependabot for dependency updates</li> <li>Secret scanning to prevent credential leaks</li> <li>SAST scanning in CI/CD pipeline</li> </ul>"},{"location":"repositories/#environment-management","title":"Environment Management","text":"<p>All repositories support three environments: development, staging, and production. See the Environment Management Guide for configuration details.</p> <p>Each repository uses: - Environment-specific <code>.env</code> files - Progressive CI/CD deployment (dev \u2192 staging \u2192 prod) - Environment-aware configuration</p>"},{"location":"repositories/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>Each repository contributes to observability: - Infrastructure: Creates CloudWatch dashboards - Pipelines: Emits Prefect metrics and custom CloudWatch metrics - dbt: Generates run artifacts and test results - Ingestion: Logs extraction metrics and errors</p> <p>All logs are centralized in CloudWatch Logs with consistent formatting and correlation IDs for tracing across services.</p>"},{"location":"roadmaps/agentic-timeline-split/","title":"Agentic Programming Timeline Split for Claude Code","text":"<p>This document provides a battle-tested approach to splitting the 6-week timeline across multiple Claude Code sessions, with realistic time estimates, dependency management, and recovery strategies.</p>"},{"location":"roadmaps/agentic-timeline-split/#core-principles","title":"Core Principles","text":"<ol> <li>Context-Focused Sessions: Each session has a single, clear objective</li> <li>Dependency-Aware: Explicit dependencies and parallel opportunities</li> <li>Test-Driven: Every session includes testing in its time estimate</li> <li>Recovery-Ready: Each session can be re-run or recovered</li> <li>Progress Tracking: Clear validation gates and success metrics</li> </ol>"},{"location":"roadmaps/agentic-timeline-split/#session-dependency-graph","title":"Session Dependency Graph","text":"<pre><code>graph TD\n    S0[Session 0: Preparation]\n    S1[S1: Local Dev Environment]\n    S2[S2: Pulumi Structure]\n    S3[S3: Core AWS]\n    S4[S4: Repository Setup]\n\n    S0 --&gt; S1\n    S0 --&gt; S2\n    S2 --&gt; S3\n    S1 --&gt; S4\n\n    S5[S5: ECS Infrastructure]\n    S6[S6: RDS &amp; Networking]\n    S7[S7: CI/CD Pipelines]\n\n    S3 --&gt; S5\n    S3 --&gt; S6\n    S4 --&gt; S7\n\n    S8[S8: Prefect Server]\n    S9[S9: Prefect Agent]\n    S10[S10: Prefect Blocks]\n\n    S5 --&gt; S8\n    S6 --&gt; S8\n    S8 --&gt; S9\n    S9 --&gt; S10\n\n    S11[S11: Monitoring]\n    S12[S12: dbt on ECS]\n    S13[S13: Glue Setup]\n    S14[S14: Data Models]\n\n    S8 --&gt; S11\n    S5 --&gt; S12\n    S3 --&gt; S13\n    S12 --&gt; S14\n\n    S15[S15: FMP Connector]\n    S16[S16: Orchestration]\n    S17[S17: Data Quality]\n    S18[S18: Backfill]\n\n    S10 --&gt; S16\n    S14 --&gt; S16\n    S15 --&gt; S16\n    S16 --&gt; S17\n    S16 --&gt; S18\n\n    S19[S19: API Layer]\n    S20[S20: Advanced Monitoring]\n    S21[S21: Security]\n    S22[S22: Documentation]\n\n    S16 --&gt; S19\n    S11 --&gt; S20\n    S19 --&gt; S21\n    S21 --&gt; S22\n\n    style S0 fill:#f9f,stroke:#333,stroke-width:4px\n    style S16 fill:#9f9,stroke:#333,stroke-width:4px</code></pre>"},{"location":"roadmaps/agentic-timeline-split/#progress-tracking-system","title":"Progress Tracking System","text":""},{"location":"roadmaps/agentic-timeline-split/#session-status-indicators","title":"Session Status Indicators","text":"<ul> <li>\ud83d\udd34 Blocked: Missing dependencies</li> <li>\ud83d\udfe1 Ready: Dependencies met, can start</li> <li>\ud83d\udfe2 Complete: Finished with tests passing</li> <li>\ud83d\udd35 In Progress: Currently active</li> <li>\u26aa Optional: Can be deferred</li> </ul>"},{"location":"roadmaps/agentic-timeline-split/#validation-gates","title":"Validation Gates","text":"<p>Each week ends with a validation gate before proceeding: - Week 1 Gate: Local dev works, core AWS deployed - Week 2 Gate: Full infrastructure deployed - Week 3 Gate: Prefect operational - Week 4 Gate: Transformation infrastructure ready - Week 5 Gate: End-to-end pipeline working - Week 6 Gate: Production ready</p>"},{"location":"roadmaps/agentic-timeline-split/#session-0-preparation-do-this-first","title":"Session 0: Preparation (Do This First!)","text":""},{"location":"roadmaps/agentic-timeline-split/#session-0-environment-and-credentials-setup","title":"\ud83d\udd34 Session 0: Environment and Credentials Setup","text":"<p>Duration: 1-2 hours (human time, not Claude time) When: Before any other session</p> <p>Human Tasks: <pre><code># 1. Set up AWS credentials\nexport AWS_ACCESS_KEY_ID=\"your-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret\"\nexport AWS_DEFAULT_REGION=\"ca-central-1\"\n\n# 2. Install required tools\nbrew install pulumi\nbrew install aws-cli\nbrew install docker\nbrew install python@3.12\n\n# 3. Create GitHub repositories (empty)\n- finks-infrastructure\n- finks-pipelines  \n- finks-dbt\n- finks-ingestion\n\n# 4. Set up Pulumi account and backend\npulumi login s3://your-pulumi-state-bucket\n\n# 5. Prepare FMP API key\nexport FMP_API_KEY=\"your-fmp-key\"\n</code></pre></p> <p>Validation: Can run <code>aws s3 ls</code> and <code>pulumi version</code> successfully</p>"},{"location":"roadmaps/agentic-timeline-split/#week-1-foundation-4-parallel-sessions","title":"Week 1: Foundation (4 Parallel Sessions)","text":""},{"location":"roadmaps/agentic-timeline-split/#session-1-local-development-environment","title":"\ud83d\udfe1 Session 1: Local Development Environment","text":"<p>Repository: <code>finks-infrastructure</code> Estimated Duration: 3-4 hours (includes testing) Dependencies: Session 0 complete</p> <p>Context Prime for Claude: <pre><code>I need you to create a complete local development environment for the Finks data pipeline.\nWe're using Docker Compose to simulate AWS services locally with LocalStack.\nFocus on developer experience - it should be easy to start/stop/reset the environment.\nInclude comprehensive error handling and helpful error messages.\n</code></pre></p> <p>Required Resources: - Docker Desktop running - 8GB RAM available - Port 4200 (Prefect), 4566 (LocalStack) free</p> <p>Specific Tasks: <pre><code>1. Docker Compose Setup (45 min)\n   - Prefect Server with PostgreSQL\n   - LocalStack with S3, Glue, Athena services\n   - Proper health checks\n   - Volume persistence\n\n2. Developer Tools (30 min)\n   - Makefile with targets:\n     - make start\n     - make stop  \n     - make reset\n     - make test\n   - .env.example file\n\n3. Testing Harness (45 min)\n   - Test script that validates all services\n   - Sample data upload to LocalStack S3\n   - Prefect connection test\n\n4. Documentation (30 min)\n   - README with troubleshooting guide\n   - Architecture diagram\n   - Common issues section\n\n5. Integration Test (30 min)\n   - Full stack startup\n   - Create test Prefect flow\n   - Upload to S3 and verify\n</code></pre></p> <p>Success Criteria: - [ ] <code>make start</code> brings up all services in &lt;2 minutes - [ ] <code>make test</code> passes all checks - [ ] Can create and run a Prefect flow locally - [ ] LocalStack S3 operations work</p> <p>Common Pitfalls: - Docker resource limits too low - Port conflicts with existing services - LocalStack Pro features attempted (use free tier only)</p> <p>Recovery Strategy: If session fails, commit working Docker Compose and continue in next session</p> <p>Template to Accelerate: <pre><code># docker-compose.yml starter\nversion: '3.8'\nservices:\n  postgres:\n    image: postgres:14-alpine\n    environment:\n      POSTGRES_USER: prefect\n      POSTGRES_PASSWORD: prefect\n      POSTGRES_DB: prefect\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U prefect\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  prefect-server:\n    image: prefecthq/prefect:2-python3.12\n    command: prefect server start\n    environment:\n      PREFECT_SERVER_API_HOST: 0.0.0.0\n      PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@postgres:5432/prefect\n    ports:\n      - \"4200:4200\"\n    depends_on:\n      postgres:\n        condition: service_healthy\n\n  localstack:\n    image: localstack/localstack:latest\n    environment:\n      SERVICES: s3,glue,athena,iam,sts\n      DEFAULT_REGION: ca-central-1\n      DATA_DIR: /tmp/localstack/data\n    ports:\n      - \"4566:4566\"\n    volumes:\n      - \"./localstack:/tmp/localstack\"\n</code></pre></p> <p>Handoff Documentation: <pre><code>## Session 1 Handoff\n- \u2705 Docker Compose working with all services\n- \u2705 Makefile with developer commands  \n- \u2705 LocalStack S3 tested and working\n- \u2705 Prefect Server accessible at http://localhost:4200\n- \ud83d\udcc1 Files created:\n  - docker-compose.yml\n  - Makefile\n  - .env.example\n  - tests/validate_stack.py\n  - README.md\n- \ud83d\udd17 Next: Can proceed with Session 2 or 4\n</code></pre></p>"},{"location":"roadmaps/agentic-timeline-split/#session-2-pulumi-project-structure","title":"\ud83d\udfe1 Session 2: Pulumi Project Structure","text":"<p>Repository: <code>finks-infrastructure</code> Estimated Duration: 4-5 hours (includes testing) Dependencies: Session 0 complete</p> <p>Context Prime for Claude: <pre><code>I need you to create a well-structured Pulumi project for AWS infrastructure.\nUse Python with strong typing and component-based architecture.\nEach component should be testable in isolation.\nFollow Pulumi best practices for resource naming and organization.\nThe goal is a maintainable, extensible infrastructure codebase.\n</code></pre></p> <p>Required Resources: - Pulumi CLI installed and logged in - Python 3.12+ environment - AWS credentials configured</p> <p>Specific Tasks: <pre><code>1. Project Initialization (30 min)\n   - Pulumi new aws-python\n   - Set up virtual environment\n   - Configure pyproject.toml\n   - Add type hints dependencies\n\n2. Component Architecture (90 min)\n   - BaseComponent abstract class\n   - NetworkingComponent (VPC, subnets)\n   - StorageComponent (S3 buckets)\n   - ComputeComponent (ECS, Lambda)\n   - MonitoringComponent (CloudWatch)\n\n3. Configuration System (45 min)\n   - Environment configs (dev, staging, prod)\n   - Config validation\n   - Secret handling patterns\n\n4. Testing Framework (60 min)\n   - Unit test setup with mocks\n   - Component integration tests\n   - Pulumi testing utilities\n\n5. CI/CD Setup (45 min)\n   - GitHub Actions workflow\n   - Pulumi preview on PR\n   - ESC integration for secrets\n</code></pre></p> <p>Success Criteria: - [ ] <code>pulumi preview</code> runs without errors - [ ] All components have unit tests - [ ] Type checking passes with mypy - [ ] CI workflow triggers on PR</p> <p>Common Pitfalls: - Circular dependencies between components - Missing resource provider registrations - Incorrect typing for Pulumi outputs</p> <p>Template to Accelerate: <pre><code># components/base.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict\nimport pulumi\n\nclass BaseComponent(pulumi.ComponentResource, ABC):\n    def __init__(self, name: str, args: Dict[str, Any], opts: pulumi.ResourceOptions = None):\n        super().__init__(self._get_type(), name, {}, opts)\n        self.name = name\n        self.args = args\n        self._create_resources()\n        self.register_outputs(self._get_outputs())\n\n    @abstractmethod\n    def _get_type(self) -&gt; str:\n        \"\"\"Return the component type string.\"\"\"\n        pass\n\n    @abstractmethod\n    def _create_resources(self) -&gt; None:\n        \"\"\"Create all resources for this component.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_outputs(self) -&gt; Dict[str, Any]:\n        \"\"\"Return component outputs.\"\"\"\n        pass\n\n# components/networking.py\nfrom typing import Dict, Any\nimport pulumi_aws as aws\nfrom .base import BaseComponent\n\nclass NetworkingComponent(BaseComponent):\n    def _get_type(self) -&gt; str:\n        return \"custom:infrastructure:Networking\"\n\n    def _create_resources(self) -&gt; None:\n        # Create VPC\n        self.vpc = aws.ec2.Vpc(\n            f\"{self.name}-vpc\",\n            cidr_block=\"10.0.0.0/16\",\n            enable_dns_hostnames=True,\n            enable_dns_support=True,\n            tags={\"Name\": f\"{self.name}-vpc\"}\n        )\n\n        # Create subnets...\n</code></pre></p> <p>Handoff Documentation: <pre><code>## Session 2 Handoff\n- \u2705 Pulumi project structure created\n- \u2705 Component-based architecture implemented\n- \u2705 All components have unit tests\n- \u2705 GitHub Actions CI configured\n- \ud83d\udcc1 Files created:\n  - __main__.py\n  - components/base.py\n  - components/networking.py\n  - components/storage.py\n  - components/compute.py\n  - tests/test_components.py\n  - .github/workflows/pulumi.yml\n- \ud83d\udd17 Next: Session 3 can use these components\n</code></pre></p>"},{"location":"roadmaps/agentic-timeline-split/#session-3-core-aws-components","title":"Session 3: Core AWS Components","text":"<p>Repository: <code>finks-infrastructure</code> Duration: 4-5 hours Context: AWS resource creation</p> <pre><code>Tasks for Claude:\n1. Implement VPC with public/private subnets\n2. Create S3 buckets (bronze, silver, gold) with:\n   - Lifecycle policies\n   - Versioning\n   - Access logging\n3. Set up Secrets Manager\n4. Create base IAM roles\n5. Deploy and test in dev environment\n</code></pre> <p>Handoff: Core AWS infrastructure deployed and tested</p>"},{"location":"roadmaps/agentic-timeline-split/#session-4-repository-scaffolding","title":"Session 4: Repository Scaffolding","text":"<p>Duration: 2 hours Context: Create all repository structures</p> <pre><code>Tasks for Claude:\n1. Create finks-pipelines repository:\n   - Prefect project structure\n   - Basic flow templates\n   - Testing framework\n2. Create finks-dbt repository:\n   - dbt project initialization\n   - Medallion folder structure\n   - Docker setup for local dev\n3. Create finks-ingestion repository:\n   - Connector template\n   - Shared utilities structure\n   - Docker templates\n</code></pre> <p>Handoff: All repositories initialized with proper structure</p>"},{"location":"roadmaps/agentic-timeline-split/#week-2-infrastructure-completion-3-parallel-sessions","title":"Week 2: Infrastructure Completion (3 Parallel Sessions)","text":""},{"location":"roadmaps/agentic-timeline-split/#session-5-ecs-infrastructure","title":"Session 5: ECS Infrastructure","text":"<p>Repository: <code>finks-infrastructure</code> Duration: 4-5 hours Context: Container orchestration setup</p> <pre><code>Tasks for Claude:\n1. Create ECS cluster with Fargate\n2. Define task definitions for:\n   - Prefect Server\n   - Prefect Agent\n   - Generic worker template\n3. Set up ECR repositories\n4. Configure CloudWatch log groups\n5. Test task execution\n</code></pre> <p>Handoff: ECS infrastructure ready for services</p>"},{"location":"roadmaps/agentic-timeline-split/#session-6-rds-and-networking","title":"Session 6: RDS and Networking","text":"<p>Repository: <code>finks-infrastructure</code> Duration: 3-4 hours Context: Database and network security</p> <pre><code>Tasks for Claude:\n1. Deploy Multi-AZ RDS PostgreSQL\n2. Configure security groups:\n   - RDS access\n   - ECS tasks\n   - ALB rules\n3. Set up ALB with SSL\n4. Create NAT Gateway\n5. Test connectivity between components\n</code></pre> <p>Handoff: Secure networking and database ready</p>"},{"location":"roadmaps/agentic-timeline-split/#session-7-cicd-pipelines","title":"Session 7: CI/CD Pipelines","text":"<p>Repository: All Duration: 3-4 hours Context: GitHub Actions setup</p> <pre><code>Tasks for Claude:\n1. finks-infrastructure:\n   - Pulumi preview on PR\n   - Deploy to staging on merge\n   - Production deploy on tag\n2. Other repositories:\n   - Test workflows\n   - Build and push Docker images\n   - Basic deployment workflows\n3. Add status badges to READMEs\n</code></pre> <p>Handoff: All repositories have working CI/CD</p>"},{"location":"roadmaps/agentic-timeline-split/#week-3-prefect-deployment-4-focused-sessions","title":"Week 3: Prefect Deployment (4 Focused Sessions)","text":""},{"location":"roadmaps/agentic-timeline-split/#session-8-prefect-server-deployment-critical-path","title":"\ud83d\udfe1 Session 8: Prefect Server Deployment (Critical Path)","text":"<p>Repository: <code>finks-infrastructure</code> Estimated Duration: 6-7 hours (includes debugging) Dependencies: Sessions 5 (ECS) and 6 (RDS) complete Difficulty: \u2b50\u2b50\u2b50\u2b50\u2b50 (Most complex session)</p> <p>Context Prime for Claude: <pre><code>I need you to deploy Prefect Server on ECS Fargate with ALB.\nThis is complex - Prefect needs specific environment variables and networking.\nThe server must connect to our existing RDS PostgreSQL.\nFocus on getting a minimal working deployment first, then add features.\nExpect networking issues - be patient and systematic in debugging.\n</code></pre></p> <p>Required Resources: - ECS cluster from Session 5 - RDS instance from Session 6 - VPC and subnets configured - ACM certificate for HTTPS</p> <p>Specific Tasks: <pre><code>1. Prefect Server Image Analysis (30 min)\n   - Review Prefect Docker image requirements\n   - Identify required environment variables\n   - Plan resource allocation (CPU/memory)\n\n2. Task Definition Creation (60 min)\n   - Server container definition\n   - Proper environment variables:\n     - PREFECT_API_DATABASE_CONNECTION_URL\n     - PREFECT_SERVER_API_HOST=0.0.0.0\n     - PREFECT_API_URL for agents\n   - CloudWatch logging configuration\n   - Health check setup\n\n3. ALB Configuration (90 min)\n   - Target group with health checks\n   - ALB listener rules\n   - SSL certificate attachment\n   - Security group rules\n\n4. ECS Service Deployment (90 min)\n   - Service with desired count=1 initially\n   - Network configuration (subnets)\n   - Security group assignment\n   - Service discovery setup\n\n5. Debugging &amp; Validation (120 min)\n   - Check ECS task logs\n   - Verify database connectivity\n   - Test ALB health checks\n   - Access Prefect UI\n   - Common issues:\n     - Database connection timeouts\n     - Security group misconfigurations  \n     - Health check failures\n     - Environment variable errors\n\n6. Production Readiness (60 min)\n   - Auto-scaling configuration\n   - Multi-AZ deployment\n   - Backup strategy\n   - Monitoring setup\n</code></pre></p> <p>Success Criteria: - [ ] Prefect UI accessible via HTTPS - [ ] Can create and run a test flow - [ ] Server survives ECS task restart - [ ] Logs visible in CloudWatch - [ ] Database migrations completed</p> <p>Common Pitfalls: - Database URL format issues (use asyncpg://) - Security group doesn't allow ECS\u2192RDS - Health check path incorrect (/api/health) - Insufficient CPU/memory allocation - Missing PREFECT_API_URL for agents</p> <p>Debugging Checklist: <pre><code># 1. Check ECS task status\naws ecs describe-tasks --cluster prefect-cluster --tasks &lt;task-arn&gt;\n\n# 2. View CloudWatch logs\naws logs tail /ecs/prefect-server --follow\n\n# 3. Test database connectivity from ECS task\n# Use ECS Exec to connect to container\n\n# 4. Verify security groups\naws ec2 describe-security-groups --group-ids &lt;sg-id&gt;\n\n# 5. Check ALB target health\naws elbv2 describe-target-health --target-group-arn &lt;tg-arn&gt;\n</code></pre></p> <p>Recovery Strategy: If Prefect won't start after 2 hours of debugging: 1. Fall back to EC2 deployment temporarily 2. Or use Prefect Cloud for MVP 3. Return to ECS deployment in Phase 2</p> <p>Template to Accelerate: <pre><code># Prefect ECS task definition\nprefect_task_definition = aws.ecs.TaskDefinition(\n    \"prefect-server\",\n    family=\"prefect-server\",\n    cpu=\"1024\",\n    memory=\"2048\",\n    network_mode=\"awsvpc\",\n    requires_compatibilities=[\"FARGATE\"],\n    execution_role_arn=ecs_execution_role.arn,\n    task_role_arn=prefect_task_role.arn,\n    container_definitions=json.dumps([{\n        \"name\": \"prefect-server\",\n        \"image\": \"prefecthq/prefect:2-python3.12\",\n        \"command\": [\"prefect\", \"server\", \"start\"],\n        \"environment\": [\n            {\"name\": \"PREFECT_SERVER_API_HOST\", \"value\": \"0.0.0.0\"},\n            {\"name\": \"PREFECT_SERVER_API_PORT\", \"value\": \"4200\"},\n            {\"name\": \"PREFECT_API_DATABASE_CONNECTION_URL\", \n             \"value\": f\"postgresql+asyncpg://prefect:password@{rds_endpoint}/prefect\"},\n        ],\n        \"portMappings\": [{\n            \"containerPort\": 4200,\n            \"protocol\": \"tcp\"\n        }],\n        \"healthCheck\": {\n            \"command\": [\"CMD-SHELL\", \"curl -f http://localhost:4200/api/health || exit 1\"],\n            \"interval\": 30,\n            \"timeout\": 5,\n            \"retries\": 3\n        },\n        \"logConfiguration\": {\n            \"logDriver\": \"awslogs\",\n            \"options\": {\n                \"awslogs-group\": \"/ecs/prefect-server\",\n                \"awslogs-region\": \"ca-central-1\",\n                \"awslogs-stream-prefix\": \"prefect\"\n            }\n        }\n    }])\n)\n</code></pre></p> <p>Handoff Documentation: <pre><code>## Session 8 Handoff\n- \u2705 Prefect Server running on ECS Fargate\n- \u2705 Accessible at https://prefect.example.com\n- \u2705 Connected to RDS PostgreSQL\n- \u2705 Auto-scaling configured (1-3 tasks)\n- \u2705 CloudWatch logs working\n- \ud83d\udcc1 Resources created:\n  - ECS Task Definition: prefect-server\n  - ECS Service: prefect-server-service\n  - ALB: prefect-alb\n  - Target Group: prefect-tg\n- \u26a0\ufe0f Known issues:\n  - Cold starts take ~2 minutes\n  - Need to set PREFECT_API_URL for agents\n- \ud83d\udd17 Next: Session 9 (Prefect Agent)\n</code></pre></p>"},{"location":"roadmaps/agentic-timeline-split/#session-9-prefect-agent-setup","title":"Session 9: Prefect Agent Setup","text":"<p>Repository: <code>finks-infrastructure</code> + <code>finks-pipelines</code> Duration: 3-4 hours Context: Prefect execution layer</p> <pre><code>Tasks for Claude:\n1. Deploy Prefect Agent on ECS\n2. Configure agent with proper IAM role\n3. Create test flow in finks-pipelines\n4. Verify agent picks up and executes flow\n5. Set up CloudWatch metrics\n</code></pre> <p>Handoff: Prefect can execute flows on ECS</p>"},{"location":"roadmaps/agentic-timeline-split/#session-10-prefect-blocks-configuration","title":"Session 10: Prefect Blocks Configuration","text":"<p>Repository: <code>finks-pipelines</code> Duration: 2-3 hours Context: Prefect configuration as code</p> <pre><code>Tasks for Claude:\n1. Create blocks configuration script:\n   - S3 blocks for each zone\n   - AWS credentials block\n   - ECS task blocks\n2. Add blocks deployment to CI/CD\n3. Create utility functions for common operations\n4. Test all blocks work correctly\n</code></pre> <p>Handoff: Prefect blocks configured and reusable</p>"},{"location":"roadmaps/agentic-timeline-split/#session-11-monitoring-setup","title":"Session 11: Monitoring Setup","text":"<p>Repository: <code>finks-infrastructure</code> Duration: 3-4 hours Context: Observability infrastructure</p> <pre><code>Tasks for Claude:\n1. Create CloudWatch dashboard for:\n   - ECS metrics\n   - RDS metrics\n   - S3 metrics\n2. Set up basic alarms:\n   - ECS task failures\n   - RDS connection exhaustion\n   - High error rates\n3. Configure SNS topic for alerts\n4. Document monitoring approach\n</code></pre> <p>Handoff: Basic monitoring and alerting active</p>"},{"location":"roadmaps/agentic-timeline-split/#week-4-transformation-infrastructure-3-parallel-sessions","title":"Week 4: Transformation Infrastructure (3 Parallel Sessions)","text":""},{"location":"roadmaps/agentic-timeline-split/#session-12-dbt-on-ecs","title":"Session 12: dbt on ECS","text":"<p>Repository: <code>finks-dbt</code> + <code>finks-infrastructure</code> Duration: 4-5 hours Context: dbt containerization</p> <pre><code>Tasks for Claude:\n1. Create dbt Dockerfile with:\n   - dbt-core\n   - dbt-athena adapter\n   - Python dependencies\n2. Create ECS task definition for dbt\n3. Set up Athena connection profile\n4. Create sample staging model\n5. Test dbt run on ECS\n</code></pre> <p>Handoff: dbt can run on ECS against Athena</p>"},{"location":"roadmaps/agentic-timeline-split/#session-13-glue-infrastructure","title":"Session 13: Glue Infrastructure","text":"<p>Repository: <code>finks-infrastructure</code> + <code>finks-pipelines</code> Duration: 3-4 hours Context: AWS Glue setup</p> <pre><code>Tasks for Claude:\n1. Create Glue database and tables\n2. Write PySpark script for Bronze\u2192Silver\n3. Create Glue job definition in Pulumi\n4. Add Glue crawler configuration\n5. Test job execution\n</code></pre> <p>Handoff: Glue can process Bronze\u2192Silver</p>"},{"location":"roadmaps/agentic-timeline-split/#session-14-data-models-design","title":"Session 14: Data Models Design","text":"<p>Repository: <code>finks-dbt</code> Duration: 3-4 hours Context: Business logic modeling</p> <pre><code>Tasks for Claude:\n1. Design staging models schema\n2. Create staging models for FMP data:\n   - stg_fmp_quotes\n   - stg_fmp_company_profile\n3. Create mart model:\n   - daily_portfolio_summary\n4. Add dbt tests\n5. Generate documentation\n</code></pre> <p>Handoff: Core dbt models ready</p>"},{"location":"roadmaps/agentic-timeline-split/#week-5-end-to-end-pipeline-4-integration-sessions","title":"Week 5: End-to-End Pipeline (4 Integration Sessions)","text":""},{"location":"roadmaps/agentic-timeline-split/#session-15-fmp-ingestion-connector","title":"Session 15: FMP Ingestion Connector","text":"<p>Repository: <code>finks-ingestion</code> Duration: 4-5 hours Context: Data extraction</p> <pre><code>Tasks for Claude:\n1. Create FMP API connector:\n   - Rate limiting\n   - Error handling\n   - Schema validation\n2. Add S3 upload functionality\n3. Create comprehensive tests\n4. Dockerize the connector\n5. Test full extraction flow\n</code></pre> <p>Handoff: Working FMP connector in container</p>"},{"location":"roadmaps/agentic-timeline-split/#session-16-orchestration-flows-integration-point","title":"\ud83d\udfe1 Session 16: Orchestration Flows (Integration Point)","text":"<p>Repository: <code>finks-pipelines</code> Estimated Duration: 7-8 hours (includes testing) Dependencies: Sessions 10, 14, 15 complete Difficulty: \u2b50\u2b50\u2b50\u2b50 (Complex integration)</p> <p>Context Prime for Claude: <pre><code>I need you to create the main Prefect orchestration flows.\nThis ties together ingestion (ECS), transformation (Glue), and dbt (ECS).\nFocus on reliability - use retries, error handling, and notifications.\nMake flows idempotent so they can be re-run safely.\nThis is the heart of our pipeline - take time to get it right.\n</code></pre></p> <p>Required Resources: - Prefect Server running (Session 8) - Prefect blocks configured (Session 10) - FMP connector image in ECR (Session 15) - dbt image in ECR (Session 14) - Glue job created (Session 13)</p> <p>Specific Tasks: <pre><code>1. Base Flow Structure (60 min)\n   - Project setup with prefect.yaml\n   - Common utilities (logging, errors)\n   - Shared configuration\n   - Task retry patterns\n\n2. Ingestion Flow (90 min)\n   - ECS task execution for FMP\n   - S3 upload validation\n   - Failure notifications\n   - Parameterization for dates\n   - Idempotency checks\n\n3. Transformation Flow (90 min)\n   - Glue job trigger and monitoring\n   - Wait for completion logic\n   - Error handling for Glue failures\n   - Data quality checks\n   - Bronze\u2192Silver validation\n\n4. dbt Flow (90 min)\n   - ECS task for dbt run\n   - Model selection parameters\n   - Test result handling\n   - Artifact storage to S3\n   - Failure rollback strategy\n\n5. Master Orchestration (120 min)\n   - Flow-of-flows pattern\n   - Dependency management\n   - Conditional execution\n   - Comprehensive error handling\n   - State management\n   - Notifications (success/failure)\n\n6. End-to-End Testing (90 min)\n   - Deploy all flows\n   - Run with test data\n   - Simulate failures\n   - Verify recovery\n   - Performance testing\n</code></pre></p> <p>Success Criteria: - [ ] Ingestion flow completes successfully - [ ] Transformation flow processes data - [ ] dbt flow generates Gold data - [ ] Master flow orchestrates all steps - [ ] Failures are handled gracefully - [ ] Flows are idempotent</p> <p>Common Pitfalls: - ECS task ARN changes between environments - IAM permissions for cross-service access - Timeout settings too aggressive - Missing error handling for partial failures - State locking issues with concurrent runs</p> <p>Testing Scenarios: <pre><code># Test 1: Happy path\npython -m pytest tests/test_flows.py::test_happy_path\n\n# Test 2: Ingestion failure\npython -m pytest tests/test_flows.py::test_ingestion_failure\n\n# Test 3: Glue job timeout\npython -m pytest tests/test_flows.py::test_glue_timeout\n\n# Test 4: dbt test failures\npython -m pytest tests/test_flows.py::test_dbt_quality_gate\n\n# Test 5: Concurrent execution\npython -m pytest tests/test_flows.py::test_concurrent_runs\n</code></pre></p> <p>Template to Accelerate: <pre><code># flows/master_flow.py\nfrom prefect import flow, task\nfrom prefect_aws.ecs import ECSTask\nfrom prefect_aws.glue import GlueJob\nfrom prefect.deployments import Deployment\nfrom prefect.server.schemas.schedules import CronSchedule\nimport boto3\nfrom datetime import datetime, timedelta\n\n@task(retries=3, retry_delay_seconds=60)\ndef run_ingestion(date: datetime) -&gt; str:\n    \"\"\"Run FMP ingestion for specific date.\"\"\"\n    ecs_task = ECSTask.load(\"fmp-ingestion-task\")\n\n    # Check if data already exists (idempotency)\n    s3 = boto3.client('s3')\n    prefix = f\"bronze/fmp/date={date.strftime('%Y-%m-%d')}/\"\n    existing = s3.list_objects_v2(\n        Bucket='finks-bronze',\n        Prefix=prefix,\n        MaxKeys=1\n    )\n\n    if existing.get('KeyCount', 0) &gt; 0:\n        print(f\"Data already exists for {date}, skipping ingestion\")\n        return \"skipped\"\n\n    # Run ingestion\n    result = ecs_task.run(\n        task_definition_arn=\"arn:aws:ecs:region:account:task-definition/fmp-ingestion\",\n        overrides={\n            \"containerOverrides\": [{\n                \"name\": \"fmp-connector\",\n                \"environment\": [\n                    {\"name\": \"EXTRACTION_DATE\", \"value\": date.strftime('%Y-%m-%d')},\n                    {\"name\": \"S3_BUCKET\", \"value\": \"finks-bronze\"}\n                ]\n            }]\n        },\n        wait_for_completion=True,\n        poll_interval=10\n    )\n    return \"completed\"\n\n@task(retries=2, retry_delay_seconds=120)\ndef run_glue_transformation(date: datetime) -&gt; str:\n    \"\"\"Run Glue job for Bronze to Silver.\"\"\"\n    glue_job = GlueJob.load(\"bronze-to-silver-job\")\n\n    run = glue_job.run(\n        job_name=\"bronze-to-silver\",\n        arguments={\n            \"--DATE\": date.strftime('%Y-%m-%d'),\n            \"--SOURCE_BUCKET\": \"finks-bronze\",\n            \"--TARGET_BUCKET\": \"finks-silver\"\n        }\n    )\n\n    # Wait for completion\n    run.wait_for_completion()\n\n    if run.status != \"SUCCEEDED\":\n        raise Exception(f\"Glue job failed: {run.status}\")\n\n    return \"completed\"\n\n@task(retries=2, retry_delay_seconds=60)\ndef run_dbt_models(date: datetime, models: str = None) -&gt; str:\n    \"\"\"Run dbt models on ECS.\"\"\"\n    ecs_task = ECSTask.load(\"dbt-task\")\n\n    command = [\"dbt\", \"run\"]\n    if models:\n        command.extend([\"--select\", models])\n\n    result = ecs_task.run(\n        task_definition_arn=\"arn:aws:ecs:region:account:task-definition/dbt-core\",\n        overrides={\n            \"containerOverrides\": [{\n                \"name\": \"dbt\",\n                \"command\": command,\n                \"environment\": [\n                    {\"name\": \"DBT_VARS\", \n                     \"value\": f'{{\"run_date\": \"{date.strftime(\"%Y-%m-%d\")}\"}}'}\n                ]\n            }]\n        },\n        wait_for_completion=True\n    )\n\n    # Run tests\n    test_result = ecs_task.run(\n        task_definition_arn=\"arn:aws:ecs:region:account:task-definition/dbt-core\",\n        overrides={\n            \"containerOverrides\": [{\n                \"name\": \"dbt\",\n                \"command\": [\"dbt\", \"test\"]\n            }]\n        },\n        wait_for_completion=True\n    )\n\n    return \"completed\"\n\n@flow(name=\"daily-data-pipeline\", retries=1)\ndef daily_pipeline(date: datetime = None):\n    \"\"\"Main orchestration flow.\"\"\"\n    if date is None:\n        date = datetime.now() - timedelta(days=1)\n\n    # Step 1: Ingestion\n    ingestion_result = run_ingestion(date)\n\n    # Step 2: Transformation (only if ingestion succeeded)\n    if ingestion_result == \"completed\":\n        glue_result = run_glue_transformation(date)\n\n        # Step 3: dbt (only if Glue succeeded)\n        if glue_result == \"completed\":\n            dbt_result = run_dbt_models(date)\n\n    return f\"Pipeline completed for {date}\"\n\n# Create deployment\nif __name__ == \"__main__\":\n    deployment = Deployment.build_from_flow(\n        flow=daily_pipeline,\n        name=\"daily-pipeline-deployment\",\n        schedule=CronSchedule(cron=\"0 2 * * *\", timezone=\"UTC\"),\n        tags=[\"production\", \"daily\"]\n    )\n    deployment.apply()\n</code></pre></p> <p>Handoff Documentation: <pre><code>## Session 16 Handoff  \n- \u2705 All Prefect flows created and tested\n- \u2705 End-to-end pipeline working\n- \u2705 Deployed with daily schedule\n- \u2705 Error handling and retries configured\n- \u2705 Idempotency implemented\n- \ud83d\udcc1 Flows created:\n  - flows/ingestion.py\n  - flows/transformation.py  \n  - flows/dbt.py\n  - flows/master_flow.py\n  - tests/test_flows.py\n- \u26a0\ufe0f Configuration needed:\n  - Update ECS task ARNs for production\n  - Configure SNS topic for notifications\n- \ud83d\udd17 Next: Session 17 (Data Quality)\n</code></pre></p>"},{"location":"roadmaps/agentic-timeline-split/#session-17-data-quality-testing","title":"Session 17: Data Quality &amp; Testing","text":"<p>Repository: <code>finks-dbt</code> + <code>finks-pipelines</code> Duration: 3-4 hours Context: Quality assurance</p> <pre><code>Tasks for Claude:\n1. Add dbt data tests:\n   - Source freshness\n   - Referential integrity\n   - Business rule validation\n2. Create data quality Prefect tasks\n3. Add quality gates to flows\n4. Create test data generator\n5. Run full pipeline test\n</code></pre> <p>Handoff: Pipeline with quality checks</p>"},{"location":"roadmaps/agentic-timeline-split/#session-18-historical-backfill","title":"Session 18: Historical Backfill","text":"<p>Repository: <code>finks-pipelines</code> Duration: 3-4 hours Context: Batch processing</p> <pre><code>Tasks for Claude:\n1. Parameterize flows for date ranges\n2. Create backfill orchestration flow\n3. Add parallel execution logic\n4. Test with 30 days of data\n5. Monitor resource usage\n</code></pre> <p>Handoff: Ability to process historical data</p>"},{"location":"roadmaps/agentic-timeline-split/#week-6-production-readiness-4-finalization-sessions","title":"Week 6: Production Readiness (4 Finalization Sessions)","text":""},{"location":"roadmaps/agentic-timeline-split/#session-19-api-development","title":"Session 19: API Development","text":"<p>Repository: <code>finks-infrastructure</code> Duration: 4-5 hours Context: Data access layer</p> <pre><code>Tasks for Claude:\n1. Create Lambda function for Athena queries\n2. Set up API Gateway:\n   - REST endpoints\n   - Request validation\n   - API key authentication\n3. Add caching layer\n4. Create OpenAPI documentation\n5. Test API thoroughly\n</code></pre> <p>Handoff: Working data access API</p>"},{"location":"roadmaps/agentic-timeline-split/#session-20-advanced-monitoring","title":"Session 20: Advanced Monitoring","text":"<p>Repository: <code>finks-infrastructure</code> + <code>finks-pipelines</code> Duration: 3-4 hours Context: Production observability</p> <pre><code>Tasks for Claude:\n1. Add custom CloudWatch metrics:\n   - Records processed\n   - Pipeline latency\n   - Data quality scores\n2. Create detailed dashboards\n3. Set up PagerDuty integration\n4. Add distributed tracing\n5. Create runbooks\n</code></pre> <p>Handoff: Comprehensive monitoring</p>"},{"location":"roadmaps/agentic-timeline-split/#session-21-security-hardening","title":"Session 21: Security Hardening","text":"<p>Repository: All Duration: 4-5 hours Context: Security review</p> <pre><code>Tasks for Claude:\n1. Review and tighten IAM policies\n2. Enable VPC endpoints\n3. Set up AWS GuardDuty\n4. Configure AWS Config rules\n5. Run security scan\n6. Document security posture\n</code></pre> <p>Handoff: Hardened security configuration</p>"},{"location":"roadmaps/agentic-timeline-split/#session-22-documentation-handoff","title":"Session 22: Documentation &amp; Handoff","text":"<p>Repository: All Duration: 3-4 hours Context: Operational readiness</p> <pre><code>Tasks for Claude:\n1. Create operational runbooks:\n   - Common issues resolution\n   - Disaster recovery steps\n   - Scaling procedures\n2. Generate architecture diagrams\n3. Create onboarding guide\n4. Document all APIs\n5. Create troubleshooting guide\n</code></pre> <p>Handoff: Complete operational documentation</p>"},{"location":"roadmaps/agentic-timeline-split/#best-practices-for-claude-code-sessions","title":"Best Practices for Claude Code Sessions","text":""},{"location":"roadmaps/agentic-timeline-split/#1-session-preparation","title":"1. Session Preparation","text":"<pre><code>Before starting each session:\n1. Clear objective and task list\n2. Links to relevant documentation\n3. Previous session's handoff notes\n4. Required credentials in environment\n</code></pre>"},{"location":"roadmaps/agentic-timeline-split/#2-context-management","title":"2. Context Management","text":"<pre><code>Help Claude maintain context:\n1. Start with: \"Continue from Session X handoff\"\n2. Provide repository structure overview\n3. Reference existing code patterns\n4. Keep sessions focused on one domain\n</code></pre>"},{"location":"roadmaps/agentic-timeline-split/#3-testing-strategy","title":"3. Testing Strategy","text":"<pre><code>Each session should:\n1. Write tests before implementation\n2. Run tests frequently\n3. Document test results\n4. Leave passing test suite\n</code></pre>"},{"location":"roadmaps/agentic-timeline-split/#4-handoff-documentation","title":"4. Handoff Documentation","text":"<pre><code>End each session with:\n1. Summary of completed work\n2. List of created/modified files\n3. Test results\n4. Next steps for continuation\n5. Any blocking issues\n</code></pre>"},{"location":"roadmaps/agentic-timeline-split/#5-parallel-work-optimization","title":"5. Parallel Work Optimization","text":"<pre><code>Week 1-2: Infrastructure sessions can run in parallel\nWeek 3-4: Repository-specific work in parallel\nWeek 5-6: Integration sessions need sequencing\n</code></pre>"},{"location":"roadmaps/agentic-timeline-split/#session-scheduling-example","title":"Session Scheduling Example","text":""},{"location":"roadmaps/agentic-timeline-split/#week-1-schedule","title":"Week 1 Schedule","text":"<ul> <li>Monday AM: Session 1 (Local Dev)</li> <li>Monday PM: Session 2 (Pulumi Structure)</li> <li>Tuesday: Session 3 (AWS Components)</li> <li>Wednesday AM: Session 4 (Repository Setup)</li> <li>Wednesday PM: Review and integration</li> </ul>"},{"location":"roadmaps/agentic-timeline-split/#daily-pattern","title":"Daily Pattern","text":"<ul> <li>Morning: Complex implementation session (4-5 hours)</li> <li>Afternoon: Testing/documentation session (2-3 hours)</li> <li>End of day: Integration and handoff prep</li> </ul>"},{"location":"roadmaps/agentic-timeline-split/#risk-mitigation","title":"Risk Mitigation","text":""},{"location":"roadmaps/agentic-timeline-split/#1-session-failures","title":"1. Session Failures","text":"<ul> <li>Each session produces incremental value</li> <li>Git commits after each working feature</li> <li>Detailed handoff notes for recovery</li> </ul>"},{"location":"roadmaps/agentic-timeline-split/#2-integration-issues","title":"2. Integration Issues","text":"<ul> <li>Dedicated integration sessions</li> <li>Test interfaces between components</li> <li>Keep sessions loosely coupled</li> </ul>"},{"location":"roadmaps/agentic-timeline-split/#3-context-loss","title":"3. Context Loss","text":"<ul> <li>Comprehensive documentation in each session</li> <li>Use git commits as checkpoints</li> <li>Reference previous work explicitly</li> </ul> <p>This approach maximizes Claude Code's effectiveness while maintaining development velocity and code quality.</p>"},{"location":"roadmaps/aggressive-timeline/","title":"Realistic MVP Timeline: Production-Ready Pipeline in Six Weeks","text":"<p>This document outlines a pragmatic, test-driven engineering plan to deliver a production-grade, end-to-end data pipeline within six weeks. The timeline includes proper testing, basic monitoring, and realistic buffers for debugging complex distributed systems.</p> <p>Guiding Principles:  1. Build incrementally with testing at each stage 2. Include basic monitoring and alerting from day one 3. Allow time for inevitable AWS/networking debugging 4. Deliver a reliable, maintainable data product</p>"},{"location":"roadmaps/aggressive-timeline/#repository-structure","title":"Repository Structure","text":"<p>The MVP will be organized across four main repositories:</p>"},{"location":"roadmaps/aggressive-timeline/#1-finks-infrastructure-pulumi-iac-for-all-aws-resources","title":"1. <code>finks-infrastructure</code> - Pulumi IaC for all AWS resources","text":"<ul> <li>Owner: Platform/DevOps Engineer</li> <li>Contents:</li> <li>Pulumi components for VPC, ECS, RDS, S3, Glue</li> <li>API Gateway and Lambda functions</li> <li>CloudWatch dashboards and alarms</li> <li>IAM roles and policies</li> <li>CI/CD: GitHub Actions \u2192 Pulumi preview/deploy</li> <li>Testing: Pulumi unit tests, policy tests</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#2-finks-pipelines-prefect-flows-and-orchestration-code","title":"2. <code>finks-pipelines</code> - Prefect flows and orchestration code","text":"<ul> <li>Owner: Data Engineer</li> <li>Contents:</li> <li>Prefect flow definitions</li> <li>Prefect blocks configuration</li> <li>AWS Glue PySpark scripts</li> <li>Orchestration utilities and helpers</li> <li>CI/CD: GitHub Actions \u2192 Deploy flows to Prefect</li> <li>Testing: Flow unit tests, integration tests</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#3-finks-dbt-dbt-models-and-transformations","title":"3. <code>finks-dbt</code> - dbt models and transformations","text":"<ul> <li>Owner: Analytics Engineer</li> <li>Contents:</li> <li>dbt models (staging, intermediate, marts)</li> <li>dbt tests and documentation</li> <li>dbt macros and packages</li> <li>Athena/Redshift profiles</li> <li>CI/CD: GitHub Actions \u2192 dbt slim CI, deploy to ECR</li> <li>Testing: dbt tests, data quality checks</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#4-finks-ingestion-containerized-data-connectors","title":"4. <code>finks-ingestion</code> - Containerized data connectors","text":"<ul> <li>Owner: Data Engineer</li> <li>Contents:</li> <li>Python ingestion scripts for each data source</li> <li>Dockerfiles for containerization</li> <li>Connection utilities and error handling</li> <li>Schema definitions and validators</li> <li>CI/CD: GitHub Actions \u2192 Build and push to ECR</li> <li>Testing: Unit tests, integration tests with mock APIs</li> </ul> <p>Each repository follows the same standards for code quality, testing, and documentation.</p>"},{"location":"roadmaps/aggressive-timeline/#six-week-mvp-plan","title":"Six-Week MVP Plan","text":""},{"location":"roadmaps/aggressive-timeline/#week-1-development-environment-basic-infrastructure","title":"Week 1: Development Environment &amp; Basic Infrastructure","text":"<p>Focus on getting the development environment right and deploying the most basic AWS resources.</p> <ul> <li>Epic: Local Development Setup </li> <li>Repository: Create new <code>finks-infrastructure</code> repository</li> <li>Task: <code>[ ]</code> Set up Python environment with uv and pyproject.toml.</li> <li>Task: <code>[ ]</code> Configure development tools (<code>black</code>, <code>ruff</code>, <code>pre-commit</code>, <code>pytest</code>) in pyproject.toml.</li> <li>Task: <code>[ ]</code> Create docker-compose.yml for local Prefect Server and PostgreSQL.</li> <li>Task: <code>[ ]</code> Set up LocalStack for S3/Glue/Athena testing.</li> <li>Task: <code>[ ]</code> Document local setup in README.md.</li> <li> <p>Test: <code>[ ]</code> Verify local Prefect can write to LocalStack S3.</p> </li> <li> <p>Epic: Pulumi Foundation</p> </li> <li>Repository: <code>finks-infrastructure</code> (same as above)</li> <li>Task: <code>[ ]</code> Initialize Pulumi project with clear component structure.</li> <li>Task: <code>[ ]</code> Configure Pulumi for <code>dev</code> and <code>staging</code> environments only (defer prod).</li> <li>Task: <code>[ ]</code> Set up environment configuration with <code>.env.dev</code> and <code>.env.staging</code> files.</li> <li>Task: <code>[ ]</code> Create S3 backend for Pulumi state with versioning and encryption.</li> <li>Task: <code>[ ]</code> Write unit tests for Pulumi components using Pulumi's testing framework.</li> <li>Test: <code>[ ]</code> Deploy and destroy a test stack successfully.</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#week-2-core-aws-infrastructure-cicd","title":"Week 2: Core AWS Infrastructure &amp; CI/CD","text":"<p>Build the foundational AWS resources with proper testing and monitoring.</p> <ul> <li>Epic: Networking &amp; Security</li> <li>Repository: <code>finks-infrastructure</code></li> <li>Task: <code>[ ]</code> Implement VPC with public/private subnets using Pulumi component.</li> <li>Task: <code>[ ]</code> Create NAT Gateway and Internet Gateway.</li> <li>Task: <code>[ ]</code> Define Security Groups for ECS tasks, RDS, and ALB.</li> <li>Task: <code>[ ]</code> Set up VPC Flow Logs to S3 for debugging.</li> <li>Test: <code>[ ]</code> Validate connectivity between subnets using EC2 test instances.</li> <li> <p>Monitor: <code>[ ]</code> CloudWatch dashboard for VPC metrics.</p> </li> <li> <p>Epic: Data Lake &amp; Secrets</p> </li> <li>Repository: <code>finks-infrastructure</code></li> <li>Task: <code>[ ]</code> Create Bronze Zone, Silver Zone, Gold Zone S3 buckets with lifecycle policies.</li> <li>Task: <code>[ ]</code> Enable S3 access logging and CloudTrail for audit.</li> <li>Task: <code>[ ]</code> Deploy AWS Secrets Manager with rotation policies.</li> <li>Task: <code>[ ]</code> Create IAM roles with least-privilege policies for each service.</li> <li>Test: <code>[ ]</code> Verify IAM policies using AWS IAM Policy Simulator.</li> <li> <p>Monitor: <code>[ ]</code> S3 bucket metrics and access patterns.</p> </li> <li> <p>Epic: CI/CD Pipeline</p> </li> <li>Repository: <code>finks-infrastructure</code> (GitHub Actions workflow)</li> <li>Task: <code>[ ]</code> GitHub Actions workflow for <code>pulumi preview</code> on PRs.</li> <li>Task: <code>[ ]</code> Staging deployment workflow with manual approval gate.</li> <li>Task: <code>[ ]</code> Add infrastructure validation tests to CI pipeline.</li> <li>Test: <code>[ ]</code> Successful PR preview and staging deployment.</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#week-3-orchestration-platform-prefect","title":"Week 3: Orchestration Platform (Prefect)","text":"<p>Dedicated week for Prefect deployment - this is complex and needs proper attention.</p> <ul> <li>Epic: RDS Database Setup</li> <li>Repository: <code>finks-infrastructure</code></li> <li>Task: <code>[ ]</code> Deploy Multi-AZ PostgreSQL RDS with encryption.</li> <li>Task: <code>[ ]</code> Configure automated backups and point-in-time recovery.</li> <li>Task: <code>[ ]</code> Set up security group for database access.</li> <li>Test: <code>[ ]</code> Connect to RDS from local environment through bastion.</li> <li> <p>Monitor: <code>[ ]</code> RDS performance insights and connection metrics.</p> </li> <li> <p>Epic: Prefect Server on ECS</p> </li> <li>Repository: <code>finks-infrastructure</code></li> <li>Task: <code>[ ]</code> Create ECS cluster with Fargate capacity providers.</li> <li>Task: <code>[ ]</code> Define Prefect Server task definition with proper resource limits.</li> <li>Task: <code>[ ]</code> Deploy ALB with health checks and SSL certificate.</li> <li>Task: <code>[ ]</code> Configure ECS service with auto-scaling policies.</li> <li>Test: <code>[ ]</code> Access Prefect UI and create test flow.</li> <li>Monitor: <code>[ ]</code> ECS task metrics and ALB target health.</li> <li> <p>Buffer: <code>[ ]</code> 2 days for debugging ECS networking issues.</p> </li> <li> <p>Epic: Prefect Agent Setup</p> </li> <li>Repository: <code>finks-infrastructure</code></li> <li>Task: <code>[ ]</code> Create separate ECS task definition for Prefect Agent.</li> <li>Task: <code>[ ]</code> Configure agent with proper IAM role for S3/ECS access.</li> <li>Task: <code>[ ]</code> Set up CloudWatch log groups for agent logs.</li> <li>Test: <code>[ ]</code> Agent successfully picks up and executes test flows.</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#week-4-data-transformation-infrastructure","title":"Week 4: Data Transformation Infrastructure","text":"<p>Focus on dbt and Glue setup with proper testing before building flows.</p> <ul> <li>Epic: dbt Development Setup</li> <li>Repository: Create new <code>finks-dbt</code> repository</li> <li>Task: <code>[ ]</code> Initialize dbt project with medallion folder structure.</li> <li>Task: <code>[ ]</code> Set up Python environment with uv and pyproject.toml including dbt-athena.</li> <li>Task: <code>[ ]</code> Create Dockerfile for dbt with all required adapters.</li> <li>Task: <code>[ ]</code> Set up dbt profiles for Athena connection.</li> <li>Task: <code>[ ]</code> Create ECS task definition for ephemeral dbt runs.</li> <li>Test: <code>[ ]</code> Local dbt run against LocalStack Athena.</li> <li> <p>Test: <code>[ ]</code> ECS task successfully runs dbt debug.</p> </li> <li> <p>Epic: AWS Glue Setup</p> </li> <li>Repository: <code>finks-infrastructure</code> (Glue resources) + <code>finks-pipelines</code> (PySpark scripts)</li> <li>Task: <code>[ ]</code> Create Glue database and crawler configurations.</li> <li>Task: <code>[ ]</code> Write PySpark script for Bronze\u2192Silver transformation.</li> <li>Task: <code>[ ]</code> Define Glue job with proper resource allocation.</li> <li>Task: <code>[ ]</code> Set up Glue job bookmarking for incremental processing.</li> <li>Test: <code>[ ]</code> Glue job processes sample data successfully.</li> <li> <p>Monitor: <code>[ ]</code> Glue job metrics and data quality metrics.</p> </li> <li> <p>Epic: Prefect Blocks as Code</p> </li> <li>Repository: Create new <code>finks-pipelines</code> repository</li> <li>Task: <code>[ ]</code> Create Python script to manage Prefect blocks.</li> <li>Task: <code>[ ]</code> Define blocks for S3 storage, AWS credentials, ECS tasks.</li> <li>Task: <code>[ ]</code> Add block deployment to CI/CD pipeline.</li> <li>Test: <code>[ ]</code> Blocks successfully created and usable in flows.</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#week-5-first-end-to-end-pipeline","title":"Week 5: First End-to-End Pipeline","text":"<p>Build the actual data pipeline components with comprehensive testing.</p> <ul> <li>Epic: Data Ingestion</li> <li>Repository: Create new <code>finks-ingestion</code> repository</li> <li>Task: <code>[ ]</code> Set up Python environment with uv and pyproject.toml.</li> <li>Task: <code>[ ]</code> Develop Python ingestion script for FMP API with retry logic.</li> <li>Task: <code>[ ]</code> Add data validation and error handling to ingestion script.</li> <li>Task: <code>[ ]</code> Create Dockerfile and push to ECR.</li> <li>Task: <code>[ ]</code> Write unit tests for ingestion logic.</li> <li>Test: <code>[ ]</code> Local ingestion writes to LocalStack S3.</li> <li>Test: <code>[ ]</code> ECS task successfully ingests sample data to Bronze.</li> <li> <p>Monitor: <code>[ ]</code> Custom CloudWatch metrics for records processed.</p> </li> <li> <p>Epic: Data Transformation Pipeline</p> </li> <li>Repository: <code>finks-pipelines</code> (Glue job) + <code>finks-dbt</code> (models)</li> <li>Task: <code>[ ]</code> Implement Bronze\u2192Silver Glue job with schema validation.</li> <li>Task: <code>[ ]</code> Create dbt staging models with source freshness tests.</li> <li>Task: <code>[ ]</code> Build mart model (daily_portfolio_summary) with documentation.</li> <li>Task: <code>[ ]</code> Add dbt tests for data quality (not_null, unique, relationships).</li> <li>Test: <code>[ ]</code> End-to-end test from Bronze to Gold with sample data.</li> <li> <p>Monitor: <code>[ ]</code> Data quality metrics dashboard.</p> </li> <li> <p>Epic: Prefect Orchestration</p> </li> <li>Repository: <code>finks-pipelines</code></li> <li>Task: <code>[ ]</code> Create modular Prefect tasks for each pipeline stage.</li> <li>Task: <code>[ ]</code> Build main flow with proper error handling and retries.</li> <li>Task: <code>[ ]</code> Implement flow notifications for failures.</li> <li>Task: <code>[ ]</code> Add data lineage tracking to DynamoDB.</li> <li>Test: <code>[ ]</code> Flow handles failures gracefully with proper rollback.</li> <li>Test: <code>[ ]</code> Successful nightly schedule execution.</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#week-6-monitoring-api-layer-production-readiness","title":"Week 6: Monitoring, API Layer &amp; Production Readiness","text":"<p>Final week focuses on observability, data access, and production hardening.</p> <ul> <li>Epic: Comprehensive Monitoring</li> <li>Repository: <code>finks-infrastructure</code> (CloudWatch resources)</li> <li>Task: <code>[ ]</code> Create CloudWatch dashboard for pipeline health metrics.</li> <li>Task: <code>[ ]</code> Set up SNS alerts for pipeline failures and data quality issues.</li> <li>Task: <code>[ ]</code> Implement distributed tracing with correlation IDs.</li> <li>Task: <code>[ ]</code> Create runbook for common operational issues.</li> <li>Test: <code>[ ]</code> Alerts fire correctly for simulated failures.</li> <li> <p>Monitor: <code>[ ]</code> End-to-end pipeline latency and success rate.</p> </li> <li> <p>Epic: Data Access API</p> </li> <li>Repository: <code>finks-infrastructure</code> (API Gateway + Lambda)</li> <li>Task: <code>[ ]</code> Create Lambda function to query Gold zone via Athena.</li> <li>Task: <code>[ ]</code> Implement API Gateway with request/response validation.</li> <li>Task: <code>[ ]</code> Add authentication with API keys and usage plans.</li> <li>Task: <code>[ ]</code> Create OpenAPI documentation for the API.</li> <li>Test: <code>[ ]</code> API returns correct data with proper error handling.</li> <li> <p>Test: <code>[ ]</code> Rate limiting and authentication work correctly.</p> </li> <li> <p>Epic: Production Readiness</p> </li> <li>Repository: All repositories (documentation and testing)</li> <li>Task: <code>[ ]</code> Perform load testing on the pipeline with 30 days of historical data.</li> <li>Task: <code>[ ]</code> Document disaster recovery procedures.</li> <li>Task: <code>[ ]</code> Create operational playbook for on-call engineers.</li> <li>Task: <code>[ ]</code> Implement backup strategy for critical components.</li> <li>Task: <code>[ ]</code> Security review of IAM policies and network configuration.</li> <li>Test: <code>[ ]</code> Full disaster recovery drill.</li> <li>Buffer: <code>[ ]</code> 2 days for final bug fixes and documentation.</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#key-success-factors","title":"Key Success Factors","text":"<ol> <li>Dedicated Team: This timeline assumes 2-3 experienced engineers working full-time</li> <li>AWS Experience: Team should have prior experience with ECS, Glue, and networking</li> <li>Quick Decision Making: Avoid analysis paralysis - make decisions and iterate</li> <li>Daily Standups: Quick sync to unblock issues immediately</li> <li>Incremental Delivery: Each week should produce working, tested components</li> </ol>"},{"location":"roadmaps/aggressive-timeline/#what-is-deferred-post-mvp","title":"What is Deferred (Post-MVP)","text":"<p>This pragmatic timeline focuses on delivering a reliable MVP. The following features are explicitly deferred:</p>"},{"location":"roadmaps/aggressive-timeline/#phase-2-weeks-7-8","title":"Phase 2 (Weeks 7-8):","text":"<ul> <li>Production Environment: Full production deployment with enhanced security</li> <li>Advanced Monitoring: Grafana dashboards and custom metrics</li> <li>Cost Optimization: Fargate Spot, VPC Endpoints, S3 lifecycle tuning</li> <li>Additional Data Sources: MongoDB connector and streaming data</li> <li>Data Catalog: Business glossary and data discovery tools</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#phase-3-weeks-9-12","title":"Phase 3 (Weeks 9-12):","text":"<ul> <li>Advanced Data Quality: Full dbt-expectations integration</li> <li>Slim CI Pattern: Only test modified dbt models in PRs</li> <li>Lake Formation: Fine-grained access control</li> <li>Multi-Region: Disaster recovery across regions</li> <li>ML Platform: SageMaker integration for model training</li> <li>Self-Service: Data source onboarding automation</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#future-enhancements","title":"Future Enhancements:","text":"<ul> <li>Streaming Architecture: Full Kinesis implementation</li> <li>Real-time Dashboards: Streaming analytics with Kinesis Analytics</li> <li>Data Mesh: Domain-oriented decentralized data ownership</li> <li>FinOps: Detailed cost attribution and optimization</li> </ul>"},{"location":"roadmaps/aggressive-timeline/#risk-mitigation-strategies","title":"Risk Mitigation Strategies","text":"<ol> <li>Week 3 Complexity: Prefect on ECS is complex</li> <li>Mitigation: Consider managed Prefect Cloud for MVP if timeline slips</li> <li> <p>Have AWS Solution Architect review the design</p> </li> <li> <p>Networking Issues: VPC configuration often causes delays</p> </li> <li>Mitigation: Use AWS VPC wizard for initial setup</li> <li> <p>Keep detailed network diagrams updated</p> </li> <li> <p>IAM Permission Errors: These can consume days of debugging</p> </li> <li>Mitigation: Start with broader permissions, tighten in Phase 2</li> <li> <p>Use CloudTrail to debug permission issues</p> </li> <li> <p>dbt/Athena Integration: Less common than dbt/Snowflake</p> </li> <li>Mitigation: Prototype early in Week 1</li> <li>Have fallback plan to use Redshift Serverless</li> </ol>"},{"location":"roadmaps/aggressive-timeline/#definition-of-done-for-mvp","title":"Definition of Done for MVP","text":"<p>The MVP is complete when: 1. \u2705 FMP API data flows automatically from source to Gold zone nightly 2. \u2705 Pipeline failures generate alerts to on-call engineer 3. \u2705 API endpoint returns portfolio summary data with authentication 4. \u2705 All infrastructure is defined as code and deployable via CI/CD 5. \u2705 Basic monitoring shows pipeline health and data quality metrics 6. \u2705 Documentation exists for operating and troubleshooting the pipeline 7. \u2705 Load test proves system can handle 90 days of historical data 8. \u2705 Disaster recovery procedure tested and documented</p> <p>This timeline provides a realistic path to a production-grade data pipeline while maintaining quality and operational excellence.</p>"},{"location":"roadmaps/project-roadmap/","title":"Data Platform Implementation Roadmap","text":"<p>This document outlines the three-month engineering plan to materialize the financial data platform. The project is divided into three one-month phases, designed to deliver incremental value and build upon a solid foundation.</p>"},{"location":"roadmaps/project-roadmap/#guiding-principles","title":"Guiding Principles","text":"<ul> <li>Foundation First: Build core infrastructure, security, and automation before application logic.</li> <li>Deliver Incrementally: Complete one end-to-end data pipeline before scaling to others.</li> <li>Automate Everything: All infrastructure, configuration, and deployments should be managed through code.</li> <li>Document as We Go: Onboarding and maintenance should be straightforward.</li> </ul>"},{"location":"roadmaps/project-roadmap/#phase-1-month-1-the-foundation","title":"Phase 1 (Month 1): The Foundation","text":"<p>Goal: Establish the secure, automated, and scalable cloud foundation. By the end of this phase, the core infrastructure will be deployed via CI/CD, and the orchestration platform will be live and ready to manage workflows.</p>"},{"location":"roadmaps/project-roadmap/#epic-1-infrastructure-as-code-project-setup","title":"Epic 1: Infrastructure as Code &amp; Project Setup","text":"<ul> <li>Task: <code>[ ]</code> Initialize Pulumi project in Python with a clear directory structure (<code>components/</code>, <code>config/</code>).</li> <li>Task: <code>[ ]</code> Configure Pulumi for three environments: <code>dev</code>, <code>staging</code>, <code>prod</code> using separate stack configuration files.</li> <li>Task: <code>[ ]</code> Set up environment configuration pattern using <code>.env</code> files. See Environment Management.</li> <li>Task: <code>[ ]</code> Provision a versioned S3 bucket to store Pulumi's state remotely.</li> <li>Task: <code>[ ]</code> Set up local development quality tools: <code>black</code> for formatting, <code>ruff</code> for linting, and <code>pre-commit</code> hooks.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-2-core-cloud-infrastructure-pulumi","title":"Epic 2: Core Cloud Infrastructure (Pulumi)","text":"<ul> <li>Task: <code>[ ]</code> Networking: Implement a <code>VPC</code> component in Pulumi to define the VPC, public/private subnets, and NAT Gateways.</li> <li>Task: <code>[ ]</code> Security: Implement a <code>Security</code> component to manage Security Groups and baseline IAM Roles (e.g., <code>ecs-task-execution-role</code>).</li> <li>Task: <code>[ ]</code> Secrets: Deploy AWS Secrets Manager and establish a clear naming convention (e.g., <code>/prod/prefect/database_url</code>). Store initial secrets.</li> <li>Task: <code>[ ]</code> Data Lake Storage: Implement a <code>DataLake</code> component to create and configure the <code>bronze-zone</code>, <code>silver-zone</code>, and <code>gold-zone</code> S3 buckets with versioning, access logging, and lifecycle policies.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-3-cicd-automation-github-actions","title":"Epic 3: CI/CD Automation (GitHub Actions)","text":"<ul> <li>Task: <code>[ ]</code> Create a GitHub Actions workflow that runs <code>pulumi preview</code> on every pull request to validate infrastructure changes.</li> <li>Task: <code>[ ]</code> Configure the workflow to run <code>pulumi up --yes</code> on merges to <code>main</code>, targeting the <code>staging</code> environment.</li> <li>Task: <code>[ ]</code> Add a manual approval gate (<code>environment: production</code>) in the workflow for all deployments to the <code>production</code> environment.</li> <li>Task: <code>[ ]</code> Build and publish a common base Docker image to AWS ECR for our Python services to ensure consistent dependencies.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-4-orchestration-platform-deployment-prefect","title":"Epic 4: Orchestration Platform Deployment (Prefect)","text":"<ul> <li>Task: <code>[ ]</code> Implement a Pulumi component to deploy Prefect Server on ECS Fargate, including task definitions and service configuration.</li> <li>Task: <code>[ ]</code> Provision a Multi-AZ PostgreSQL RDS instance for the Prefect backend.</li> <li>Task: <code>[ ]</code> Configure an Application Load Balancer (ALB) with an ACM certificate to provide secure HTTPS access to the Prefect UI.</li> <li>Task: <code>[ ]</code> Implement a Python script to manage Prefect Blocks as Code, defining S3 storage and AWS credentials blocks, and run it via CI/CD.</li> </ul>"},{"location":"roadmaps/project-roadmap/#phase-2-month-2-the-first-end-to-end-pipeline","title":"Phase 2 (Month 2): The First End-to-End Pipeline","text":"<p>Goal: Deliver the first tangible value by building a complete, production-grade data pipeline for a single batch data source (Financial Modeling Prep API). This creates a reusable pattern for all future pipelines.</p>"},{"location":"roadmaps/project-roadmap/#epic-1-batch-ingestion-connector-fmp-api","title":"Epic 1: Batch Ingestion Connector (FMP API)","text":"<ul> <li>Task: <code>[ ]</code> Define the data schema for the target API endpoint.</li> <li>Task: <code>[ ]</code> Develop a robust Python extraction script with error handling, retries, and parameterization (e.g., for dates).</li> <li>Task: <code>[ ]</code> Containerize the script and create a Prefect flow (<code>fmp-api-ingestion</code>) that runs the extraction task and lands the raw JSON data in the <code>bronze-zone</code> with proper partitioning (<code>source=fmp/year=...</code>).</li> <li>Task: <code>[ ]</code> Define the <code>ECSTask</code> resource in Pulumi for this connector, specifying its memory/CPU and IAM permissions.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-2-bronze-to-silver-cleansing-validation","title":"Epic 2: Bronze-to-Silver Cleansing &amp; Validation","text":"<ul> <li>Task: <code>[ ]</code> Write an AWS Glue script (PySpark) that reads the raw FMP data, applies schema validation, casts data types, and writes the output as Parquet to the <code>silver-zone</code>.</li> <li>Task: <code>[ ]</code> Define the Glue Job as a Pulumi resource.</li> <li>Task: <code>[ ]</code> Create a Prefect flow (<code>bronze-to-silver-fmp</code>) that triggers the Glue Job. Use an S3 event on the Bronze Zone bucket as the trigger for this flow.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-3-business-transformation-with-dbt","title":"Epic 3: Business Transformation with dbt","text":"<ul> <li>Task: <code>[ ]</code> Initialize the <code>finks-dbt</code> repository and set up the <code>profiles.yml</code> to connect to AWS Athena.</li> <li>Task: <code>[ ]</code> Create a Docker image for the dbt project and define an ephemeral dbt ECS task runner in Pulumi.</li> <li>Task: <code>[ ]</code> Build a staging model (<code>stg_fmp_market_data.sql</code>) that cleans and prepares data from the Silver table.</li> <li>Task: <code>[ ]</code> Build a mart model (<code>daily_portfolio_summary.sql</code>) that creates a business-ready, aggregated table in the <code>gold-zone</code>.</li> <li>Task: <code>[ ]</code> Add basic dbt tests (<code>not_null</code>, <code>unique</code>) to the primary keys in the dbt models.</li> <li>Task: <code>[ ]</code> Document the columns and description for the new models in their <code>schema.yml</code> file.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-4-full-end-to-end-orchestration","title":"Epic 4: Full End-to-End Orchestration","text":"<ul> <li>Task: <code>[ ]</code> Create a master Prefect flow (<code>master-fmp-pipeline</code>) that chains the ingestion, cleansing, and dbt transformation flows in the correct dependency order.</li> <li>Task: <code>[ ]</code> Implement basic success and failure notifications to a Slack channel from the master flow.</li> <li>Task: <code>[ ]</code> Schedule the master flow to run nightly using a <code>CronSchedule</code>.</li> <li>Task: <code>[ ]</code> Implement the parameterized backfill flow pattern to enable reprocessing of historical data.</li> </ul>"},{"location":"roadmaps/project-roadmap/#phase-3-month-3-scale-harden-serve","title":"Phase 3 (Month 3): Scale, Harden &amp; Serve","text":"<p>Goal: Expand the platform by adding new data types, implementing robust monitoring and data governance, and exposing data to end-users.</p>"},{"location":"roadmaps/project-roadmap/#epic-1-add-a-streaming-data-source-eg-news-feed","title":"Epic 1: Add a Streaming Data Source (e.g., News Feed)","text":"<ul> <li>Task: <code>[ ]</code> Define Kinesis Data Streams and Kinesis Firehose resources in Pulumi to create a ingestion path to the <code>bronze-zone</code>.</li> <li>Task: <code>[ ]</code> Develop and containerize the news streaming producer application and deploy it as a long-running ECS service.</li> <li>Task: <code>[ ]</code> Adapt the Bronze-to-Silver process to handle the new streaming data, including a small-file compaction strategy.</li> <li>Task: <code>[ ]</code> Build new dbt models for the news data to demonstrate joining batch and stream sources.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-2-advanced-data-quality-governance","title":"Epic 2: Advanced Data Quality &amp; Governance","text":"<ul> <li>Task: <code>[ ]</code> Integrate the <code>dbt-expectations</code> package and add more comprehensive data quality tests to all dbt models.</li> <li>Task: <code>[ ]</code> Configure the master Prefect flow to halt and alert if <code>dbt test</code> fails, creating a true Quality Gate.</li> <li>Task: <code>[ ]</code> Implement the \"Slim CI\" pattern for dbt in GitHub Actions to only build and test modified models and their downstream dependencies in PRs.</li> <li>Task: <code>[ ]</code> Use AWS Lake Formation to apply fine-grained, column-level access controls to tables in the Gold zone.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-3-observability-cost-optimization","title":"Epic 3: Observability &amp; Cost Optimization","text":"<ul> <li>Task: <code>[ ]</code> Create a unified Grafana dashboard to monitor key platform metrics (Prefect success rates, data freshness, ECS utilization, Athena query costs).</li> <li>Task: <code>[ ]</code> Implement correlated logging by injecting the <code>prefect.flow_run_id</code> into all ECS and Glue task logs, enabling end-to-end tracing.</li> <li>Task: <code>[ ]</code> Configure Prefect agent pools to run on Fargate Spot to reduce compute costs.</li> <li>Task: <code>[ ]</code> Configure VPC Endpoints for S3 and Glue to reduce NAT Gateway data transfer costs.</li> </ul>"},{"location":"roadmaps/project-roadmap/#epic-4-data-serving-layer-documentation","title":"Epic 4: Data Serving Layer &amp; Documentation","text":"<ul> <li>Task: <code>[ ]</code> Implement an API Gateway with a Lambda authorizer to provide secure access to data.</li> <li>Task: <code>[ ]</code> Create a \"serverless API\" Lambda function that queries the Gold zone via Athena to serve the <code>daily_portfolio_summary</code> dataset.</li> <li>Task: <code>[ ]</code> Create a \"Cookbook\" page in the documentation site detailing the step-by-step process for onboarding a new data source.</li> <li>Task: <code>[ ]</code> Document the local development stack (<code>docker-compose.yml</code>) and workflow for new developers.</li> </ul>"},{"location":"roadmaps/agentic-sessions/","title":"Agentic Programming Sessions for Claude Code","text":"<p>This guide provides a battle-tested approach to building the Finks data pipeline using Claude Code across 22 focused sessions over 6 weeks.</p>"},{"location":"roadmaps/agentic-sessions/#core-principles","title":"\ud83c\udfaf Core Principles","text":"<ol> <li>Context-Focused Sessions: Each session has a single, clear objective</li> <li>Dependency-Aware: Explicit dependencies and parallel opportunities</li> <li>Test-Driven: Every session includes testing in its time estimate</li> <li>Recovery-Ready: Each session can be re-run or recovered</li> <li>Progress Tracking: Clear validation gates and success metrics</li> </ol>"},{"location":"roadmaps/agentic-sessions/#session-overview","title":"\ud83d\udcca Session Overview","text":"Week Sessions Focus Validation Gate 0 Preparation Environment setup AWS access working 1 Sessions 1-4 Local dev, Pulumi, core AWS Infrastructure deployed 2 Sessions 5-7 ECS, RDS, CI/CD Full infrastructure ready 3 Sessions 8-11 Prefect deployment Prefect operational 4 Sessions 12-14 dbt, Glue, data models Transformation ready 5 Sessions 15-18 End-to-end pipeline Pipeline working 6 Sessions 19-22 API, monitoring, security Production ready"},{"location":"roadmaps/agentic-sessions/#session-dependency-graph","title":"\ud83d\udd04 Session Dependency Graph","text":"<pre><code>graph TD\n    S0[Session 0: Preparation]\n    S1[S1: Local Dev Environment]\n    S2[S2: Pulumi Structure]\n    S3[S3: Core AWS]\n    S4[S4: Repository Setup]\n\n    S0 --&gt; S1\n    S0 --&gt; S2\n    S2 --&gt; S3\n    S1 --&gt; S4\n\n    S5[S5: ECS Infrastructure]\n    S6[S6: RDS &amp; Networking]\n    S7[S7: CI/CD Pipelines]\n\n    S3 --&gt; S5\n    S3 --&gt; S6\n    S4 --&gt; S7\n\n    S8[S8: Prefect Server]\n    S9[S9: Prefect Agent]\n    S10[S10: Prefect Blocks]\n\n    S5 --&gt; S8\n    S6 --&gt; S8\n    S8 --&gt; S9\n    S9 --&gt; S10\n\n    S11[S11: Monitoring]\n    S12[S12: dbt on ECS]\n    S13[S13: Glue Setup]\n    S14[S14: Data Models]\n\n    S8 --&gt; S11\n    S5 --&gt; S12\n    S3 --&gt; S13\n    S12 --&gt; S14\n\n    S15[S15: FMP Connector]\n    S16[S16: Orchestration]\n    S17[S17: Data Quality]\n    S18[S18: Backfill]\n\n    S10 --&gt; S16\n    S14 --&gt; S16\n    S15 --&gt; S16\n    S16 --&gt; S17\n    S16 --&gt; S18\n\n    S19[S19: API Layer]\n    S20[S20: Advanced Monitoring]\n    S21[S21: Security]\n    S22[S22: Documentation]\n\n    S16 --&gt; S19\n    S11 --&gt; S20\n    S19 --&gt; S21\n    S21 --&gt; S22\n\n    style S0 fill:#f9f,stroke:#333,stroke-width:4px\n    style S16 fill:#9f9,stroke:#333,stroke-width:4px</code></pre>"},{"location":"roadmaps/agentic-sessions/#progress-tracking","title":"\ud83d\udcc8 Progress Tracking","text":""},{"location":"roadmaps/agentic-sessions/#session-status-indicators","title":"Session Status Indicators","text":"<ul> <li>\ud83d\udd34 Blocked: Missing dependencies</li> <li>\ud83d\udfe1 Ready: Dependencies met, can start</li> <li>\ud83d\udfe2 Complete: Finished with tests passing</li> <li>\ud83d\udd35 In Progress: Currently active</li> <li>\u26aa Optional: Can be deferred</li> </ul>"},{"location":"roadmaps/agentic-sessions/#progress-checklist","title":"Progress Checklist","text":"<pre><code>## Week 0\n- [ ] Session 0: Environment setup\n\n## Week 1  \n- [ ] Session 1: Local development environment\n- [ ] Session 2: Pulumi project structure\n- [ ] Session 3: Core AWS components\n- [ ] Session 4: Repository scaffolding\n\n## Week 2\n- [ ] Session 5: ECS infrastructure\n- [ ] Session 6: RDS and networking\n- [ ] Session 7: CI/CD pipelines\n\n## Week 3\n- [ ] Session 8: Prefect server deployment\n- [ ] Session 9: Prefect agent setup\n- [ ] Session 10: Prefect blocks configuration\n- [ ] Session 11: Monitoring setup\n\n## Week 4\n- [ ] Session 12: dbt on ECS\n- [ ] Session 13: Glue infrastructure\n- [ ] Session 14: Data models design\n\n## Week 5\n- [ ] Session 15: FMP ingestion connector\n- [ ] Session 16: Orchestration flows\n- [ ] Session 17: Data quality &amp; testing\n- [ ] Session 18: Historical backfill\n\n## Week 6\n- [ ] Session 19: API development\n- [ ] Session 20: Advanced monitoring\n- [ ] Session 21: Security hardening\n- [ ] Session 22: Documentation &amp; handoff\n</code></pre>"},{"location":"roadmaps/agentic-sessions/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Complete Week 0 Preparation - Set up your environment</li> <li>Review the dependency graph - Understand session relationships</li> <li>Start with Week 1 - Follow sessions in order within each week</li> <li>Track your progress - Use the checklist above</li> <li>Save session outputs - Each session should produce git commits</li> </ol>"},{"location":"roadmaps/agentic-sessions/#best-practices","title":"\ud83d\udca1 Best Practices","text":""},{"location":"roadmaps/agentic-sessions/#before-each-session","title":"Before Each Session","text":"<ol> <li>Review the session's dependencies</li> <li>Prepare the context prime for Claude</li> <li>Ensure required resources are available</li> <li>Have the previous session's handoff ready</li> </ol>"},{"location":"roadmaps/agentic-sessions/#during-each-session","title":"During Each Session","text":"<ol> <li>Start with the context prime</li> <li>Follow the specific tasks in order</li> <li>Test frequently</li> <li>Commit working code often</li> </ol>"},{"location":"roadmaps/agentic-sessions/#after-each-session","title":"After Each Session","text":"<ol> <li>Document what was completed</li> <li>Note any issues or blockers</li> <li>Prepare handoff for next session</li> <li>Update the progress checklist</li> </ol>"},{"location":"roadmaps/agentic-sessions/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"roadmaps/agentic-sessions/#common-issues","title":"Common Issues","text":"<ul> <li>Session taking too long: Break it into smaller parts</li> <li>Dependencies not met: Review the dependency graph</li> <li>Claude losing context: Use the handoff documentation</li> <li>Tests failing: Focus on one test at a time</li> </ul>"},{"location":"roadmaps/agentic-sessions/#recovery-strategies","title":"Recovery Strategies","text":"<ul> <li>Each session is designed to be re-runnable</li> <li>Use git branches for experimental changes</li> <li>Keep detailed notes on what worked</li> <li>Don't hesitate to adjust time estimates</li> </ul>"},{"location":"roadmaps/agentic-sessions/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Original 6-Week Timeline</li> <li>Repository Structure</li> <li>Architecture Overview</li> <li>Best Practices Guide</li> </ul>"},{"location":"roadmaps/agentic-sessions/best-practices/","title":"Best Practices for Claude Code Sessions","text":"<p>This guide provides proven strategies for maximizing productivity and success when using Claude Code for the Finks data pipeline implementation.</p>"},{"location":"roadmaps/agentic-sessions/best-practices/#session-planning","title":"\ud83c\udfaf Session Planning","text":""},{"location":"roadmaps/agentic-sessions/best-practices/#before-starting-any-session","title":"Before Starting Any Session","text":"<ol> <li> <p>Review Dependencies <pre><code>- [ ] Previous session handoffs reviewed\n- [ ] Required resources available\n- [ ] Repository cloned and ready\n- [ ] Environment variables set\n- [ ] Dependencies installed\n</code></pre></p> </li> <li> <p>Prepare Context Prime</p> </li> <li>Write a clear, specific objective</li> <li>Include relevant background</li> <li>Specify constraints and requirements</li> <li> <p>Reference existing patterns</p> </li> <li> <p>Set Time Expectations</p> </li> <li>Add 30% buffer to estimates</li> <li>Plan for testing time</li> <li>Account for debugging</li> <li>Include documentation time</li> </ol>"},{"location":"roadmaps/agentic-sessions/best-practices/#optimal-session-structure","title":"Optimal Session Structure","text":"<pre><code>1. Context Setting (5-10 min)\n   - Provide context prime\n   - Share relevant files\n   - Specify success criteria\n\n2. Implementation (60-80%)\n   - Focus on one component\n   - Test frequently with uv run pytest\n   - Commit working code\n\n3. Testing &amp; Validation (15-20%)\n   - Run all tests: uv run pytest\n   - Validate success criteria\n   - Fix any issues\n\n4. Documentation (5-10%)\n   - Update handoff notes\n   - Document decisions\n   - List next steps\n</code></pre>"},{"location":"roadmaps/agentic-sessions/best-practices/#working-with-claude-code","title":"\ud83d\udcbb Working with Claude Code","text":""},{"location":"roadmaps/agentic-sessions/best-practices/#effective-context-primes","title":"Effective Context Primes","text":"<p>Good Example: <pre><code>I need to deploy Prefect Server on ECS Fargate.\nPrevious session created ECS cluster and RDS database.\nUse existing VPC (vpc-xxx) and subnets from outputs.json.\nFocus on minimal working deployment first.\nServer needs to connect to RDS at endpoint: xxx.rds.amazonaws.com\nInclude health checks and CloudWatch logging.\n</code></pre></p> <p>Poor Example: <pre><code>Set up Prefect.\n</code></pre></p>"},{"location":"roadmaps/agentic-sessions/best-practices/#managing-long-sessions","title":"Managing Long Sessions","text":"<p>For sessions over 4 hours:</p> <ol> <li> <p>Break into Parts <pre><code>Part 1: Basic implementation (2 hours)\nPart 2: Testing and debugging (1.5 hours)\nPart 3: Production features (1.5 hours)\n</code></pre></p> </li> <li> <p>Use Checkpoints</p> </li> <li>Commit after each working feature</li> <li>Document progress in comments</li> <li> <p>Save outputs for reference</p> </li> <li> <p>Handle Context Loss</p> </li> <li>If Claude forgets context, provide summary</li> <li>Reference specific files again</li> <li>Use handoff documentation</li> </ol>"},{"location":"roadmaps/agentic-sessions/best-practices/#code-quality-standards","title":"Code Quality Standards","text":"<ol> <li>Always Include:</li> <li>Type hints for Python</li> <li>Error handling</li> <li>Logging statements</li> <li>Unit tests</li> <li> <p>Documentation</p> </li> <li> <p>Follow Patterns: <pre><code># Consistent error handling\ntry:\n    result = risky_operation()\nexcept SpecificError as e:\n    logger.error(f\"Operation failed: {e}\")\n    raise CustomError(f\"Failed to process: {e}\") from e\n</code></pre></p> </li> <li> <p>Testing Strategy:</p> </li> <li>Write tests during implementation</li> <li>Test edge cases</li> <li>Include integration tests</li> <li>Mock external services</li> </ol>"},{"location":"roadmaps/agentic-sessions/best-practices/#recovery-strategies","title":"\ud83d\udd04 Recovery Strategies","text":""},{"location":"roadmaps/agentic-sessions/best-practices/#when-things-go-wrong","title":"When Things Go Wrong","text":"<ol> <li>Session Fails to Complete</li> <li>Commit any working code</li> <li>Document what was attempted</li> <li>Identify blockers</li> <li> <p>Plan recovery approach</p> </li> <li> <p>Tests Keep Failing</p> </li> <li>Isolate the failing component</li> <li>Simplify the test case</li> <li>Check assumptions</li> <li> <p>Review error messages carefully</p> </li> <li> <p>Integration Issues</p> </li> <li>Verify interface contracts</li> <li>Check credentials and permissions</li> <li>Review networking configuration</li> <li>Use debugging tools</li> </ol>"},{"location":"roadmaps/agentic-sessions/best-practices/#common-recovery-patterns","title":"Common Recovery Patterns","text":"<pre><code># Rollback infrastructure\npulumi stack export &gt; backup.json\npulumi destroy -y\npulumi stack import &lt; backup.json\n\n# Debug ECS tasks\naws ecs describe-tasks --cluster &lt;cluster&gt; --tasks &lt;task-arn&gt;\naws logs tail /ecs/&lt;service&gt; --follow\n\n# Test connectivity\ndocker run --rm -it amazonlinux:2 bash\n# Install tools and test from within VPC\n</code></pre>"},{"location":"roadmaps/agentic-sessions/best-practices/#handoff-documentation","title":"\ud83d\udcdd Handoff Documentation","text":""},{"location":"roadmaps/agentic-sessions/best-practices/#effective-handoff-template","title":"Effective Handoff Template","text":"<p><pre><code>## Session X Handoff\n\n### Completed \u2705\n- Main objective achieved\n- All tests passing\n- Deployed to environment\n\n### Files Modified \ud83d\udcc1\n- path/to/file1.py - Added authentication\n- path/to/file2.yml - Updated configuration\n\n### Key Decisions \ud83c\udfaf\n- Used approach X because Y\n- Deferred feature Z to Phase 2\n\n### Known Issues \u26a0\ufe0f\n- Cold start takes 2 minutes\n- Needs optimization for large datasets\n\n### Next Steps \ud83d\udd17\n- Session Y can now proceed\n- Need to configure X before Session Z\n\n### Validation Commands \ud83e\uddea\n```bash\n# Test deployment\ncurl https://api.example.com/health\n\n# Check logs\naws logs tail /aws/lambda/function\n</code></pre> <pre><code>## \ud83d\ude80 Productivity Tips\n\n### 1. Parallel Development\n\nWhen possible, run multiple sessions:\n- Infrastructure in one session\n- Application code in another\n- Documentation in a third\n\n### 2. Template Library\n\nBuild a library of templates:\n- Pulumi components\n- Prefect flow patterns\n- Docker configurations\n- CI/CD workflows\n\n### 3. Debugging Efficiently\n\n```python\n# Add debug logging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\n# Use breakpoints\nimport pdb; pdb.set_trace()\n\n# Time operations\nimport time\nstart = time.time()\noperation()\nlogger.info(f\"Operation took {time.time() - start:.2f}s\")\n</code></pre></p>"},{"location":"roadmaps/agentic-sessions/best-practices/#4-environment-management","title":"4. Environment Management","text":"<pre><code># Save environment state\nenv &gt; session_x_env.txt\n\n# Create session-specific configs\ncat &gt; .env.session_x &lt;&lt; EOF\nSESSION_VAR=value\nEOF\n\n# Use direnv for automatic loading\necho \"source .env.session_x\" &gt; .envrc\n</code></pre>"},{"location":"roadmaps/agentic-sessions/best-practices/#learning-from-sessions","title":"\ud83c\udf93 Learning from Sessions","text":""},{"location":"roadmaps/agentic-sessions/best-practices/#post-session-review","title":"Post-Session Review","text":"<p>After each session: 1. What went well? 2. What took longer than expected? 3. What would you do differently? 4. What patterns emerged?</p>"},{"location":"roadmaps/agentic-sessions/best-practices/#building-expertise","title":"Building Expertise","text":"<ul> <li>Document solutions to tricky problems</li> <li>Create runbooks for complex procedures</li> <li>Share learnings with team</li> <li>Update estimates based on experience</li> </ul>"},{"location":"roadmaps/agentic-sessions/best-practices/#continuous-improvement","title":"Continuous Improvement","text":"<pre><code>## Session Retrospective\n\n**Duration**: Estimated 4h, Actual 5.5h\n\n**Challenges**:\n- ECS networking took extra time\n- Health check configuration was tricky\n\n**Solutions Found**:\n- Use ALB health checks, not ECS\n- Security group needs explicit egress\n\n**For Next Time**:\n- Add 2h buffer for ECS sessions\n- Prepare networking diagram first\n</code></pre>"},{"location":"roadmaps/agentic-sessions/best-practices/#quick-reference","title":"\ud83d\udd17 Quick Reference","text":""},{"location":"roadmaps/agentic-sessions/best-practices/#session-checklist","title":"Session Checklist","text":"<pre><code>## Pre-Session\n\n- [ ] Dependencies verified\n- [ ] Context prime prepared\n- [ ] Previous handoff reviewed\n- [ ] Time allocated with buffer\n\n## During Session\n\n- [ ] Following single objective\n- [ ] Testing frequently\n- [ ] Committing working code\n- [ ] Documenting decisions\n\n## Post-Session\n\n- [ ] All code committed\n- [ ] Tests passing\n- [ ] Handoff documented\n- [ ] Next steps clear\n</code></pre>"},{"location":"roadmaps/agentic-sessions/best-practices/#useful-commands","title":"Useful Commands","text":"<pre><code># AWS debugging\naws sts get-caller-identity\naws ec2 describe-instances --filters \"Name=tag:Project,Values=finks\"\naws logs tail &lt;log-group&gt; --follow --since 1h\n\n# Docker debugging\ndocker ps -a\ndocker logs &lt;container&gt; --tail 100 -f\ndocker exec -it &lt;container&gt; /bin/bash\n\n# Pulumi debugging\npulumi stack output -j\npulumi refresh\npulumi preview --diff\n\n# Python debugging with uv\nuv run pytest -xvs\nuv run python -m pdb script.py\nuv run python -m cProfile -s cumulative script.py\n</code></pre>"},{"location":"roadmaps/agentic-sessions/best-practices/#emergency-contacts","title":"Emergency Contacts","text":"<p>When truly stuck: 1. Check AWS service health 2. Review CloudTrail for permission issues 3. Check service quotas 4. Consult AWS documentation 5. Use AWS Support if available</p> <p>Remember: Each session builds on previous work. Take time to do it right, and the later sessions will go more smoothly.</p>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/","title":"Week 0: Environment Preparation","text":"<p>This preparation phase must be completed before starting any Claude Code sessions. It involves setting up your local environment, AWS credentials, and creating the necessary repositories.</p>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#session-0-environment-and-credentials-setup","title":"\ud83d\udd34 Session 0: Environment and Credentials Setup","text":"<p>Duration: 1-2 hours (human time, not Claude time) When: Before any other session Type: Manual setup (no Claude needed)</p>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#prerequisites-checklist","title":"Prerequisites Checklist","text":"<p>Before starting, ensure you have:</p> <ul> <li>[ ] AWS account with appropriate permissions</li> <li>[ ] GitHub account with ability to create repositories</li> <li>[ ] macOS or Linux development machine</li> <li>[ ] At least 16GB RAM for local development</li> <li>[ ] Stable internet connection</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-1-install-required-tools","title":"Step 1: Install Required Tools","text":"<pre><code># For macOS users\nbrew install pulumi\nbrew install awscli\nbrew install docker\nbrew install git\nbrew install jq\nbrew install make\n\n# Install uv for Python environment management\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# For Linux users\n# Follow official installation guides for each tool\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-2-configure-aws-credentials","title":"Step 2: Configure AWS Credentials","text":"<pre><code># Option 1: Using AWS CLI\naws configure\n# Enter your:\n# - AWS Access Key ID\n# - AWS Secret Access Key  \n# - Default region: ca-central-1\n# - Default output format: json\n\n# Option 2: Using environment variables\nexport AWS_ACCESS_KEY_ID=\"your-access-key-id\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-access-key\"\nexport AWS_DEFAULT_REGION=\"ca-central-1\"\n\n# Verify AWS access\naws sts get-caller-identity\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-3-set-up-pulumi","title":"Step 3: Set Up Pulumi","text":"<pre><code># Create S3 bucket for Pulumi state (if not using Pulumi Cloud)\naws s3 mb s3://finks-pulumi-state-$(aws sts get-caller-identity --query Account --output text)\n\n# Configure Pulumi to use S3 backend\npulumi login s3://finks-pulumi-state-$(aws sts get-caller-identity --query Account --output text)\n\n# Verify Pulumi installation\npulumi version\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-4-create-github-repositories","title":"Step 4: Create GitHub Repositories","text":"<p>Create these empty repositories in your GitHub account:</p>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#1-finks-infrastructure","title":"1. finks-infrastructure","text":"<ul> <li>Description: \"Pulumi IaC for Finks data pipeline AWS resources\"</li> <li>Visibility: Private repository</li> <li>Initialize with README: No</li> <li>Add .gitignore: No</li> <li>Choose license: No</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#2-finks-pipelines","title":"2. finks-pipelines","text":"<ul> <li>Description: \"Prefect orchestration flows for Finks data pipeline\"</li> <li>Visibility: Private repository</li> <li>Initialize with README: No</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#3-finks-dbt","title":"3. finks-dbt","text":"<ul> <li>Description: \"dbt transformation models for Finks data pipeline\"</li> <li>Visibility: Private repository</li> <li>Initialize with README: No</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#4-finks-ingestion","title":"4. finks-ingestion","text":"<ul> <li>Description: \"Data source connectors for Finks data pipeline\"</li> <li>Visibility: Private repository</li> <li>Initialize with README: No</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-5-clone-repositories-locally","title":"Step 5: Clone Repositories Locally","text":"<pre><code># Create workspace directory\nmkdir -p ~/workspace/finks\ncd ~/workspace/finks\n\n# Clone all repositories\ngit clone git@github.com:YOUR_USERNAME/finks-infrastructure.git\ngit clone git@github.com:YOUR_USERNAME/finks-pipelines.git\ngit clone git@github.com:YOUR_USERNAME/finks-dbt.git\ngit clone git@github.com:YOUR_USERNAME/finks-ingestion.git\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-6-prepare-api-keys-and-secrets","title":"Step 6: Prepare API Keys and Secrets","text":"<pre><code># Financial Modeling Prep API key\nexport FMP_API_KEY=\"your-fmp-api-key\"\n\n# Create a secure file for secrets (don't commit this!)\n# This is a temporary global env file - each repo will have its own later\ncat &gt; ~/workspace/finks/.env &lt;&lt; EOF\nAWS_ACCESS_KEY_ID=your-access-key-id\nAWS_SECRET_ACCESS_KEY=your-secret-access-key\nAWS_DEFAULT_REGION=ca-central-1\nFMP_API_KEY=your-fmp-api-key\nPULUMI_BACKEND_URL=s3://finks-pulumi-state-YOUR_ACCOUNT_ID\nENVIRONMENT=dev\nEOF\n\n# Secure the file\nchmod 600 ~/workspace/finks/.env\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-7-configure-docker","title":"Step 7: Configure Docker","text":"<pre><code># Start Docker Desktop (macOS)\nopen -a Docker\n\n# Verify Docker is running\ndocker ps\n\n# Configure Docker resources (via Docker Desktop preferences)\n# - CPUs: 4+\n# - Memory: 8GB+\n# - Disk: 50GB+\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-8-set-up-python-environment","title":"Step 8: Set Up Python Environment","text":"<pre><code># Verify uv is installed\nuv --version\n\n# Create a new Python project in each repository\ncd ~/workspace/finks/finks-infrastructure\n\n# Initialize Python project with uv\nuv init --python 3.12\n\n# This creates a pyproject.toml file with Python 3.12\n# The pyproject.toml will be customized for each repository\n\n# Verify Python version\nuv run python --version  # Should show 3.12.x\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-9-prepare-claude-code-context","title":"Step 9: Prepare Claude Code Context","text":"<p>Create a context file for Claude Code sessions:</p> <pre><code>cat &gt; ~/workspace/finks/CONTEXT.md &lt;&lt; 'EOF'\n# Finks Data Pipeline Context\n\n## Project Overview\nBuilding a financial data pipeline with:\n- Prefect for orchestration\n- AWS for infrastructure\n- dbt for transformations\n- Docker for containerization\n\n## Repository Structure\n- finks-infrastructure: Pulumi IaC\n- finks-pipelines: Prefect flows\n- finks-dbt: dbt models\n- finks-ingestion: Data connectors\n\n## AWS Region\nca-central-1\n\n## Key Technologies\n- Python 3.12 (with uv package manager)\n- Pulumi (Python)\n- Prefect 2.x\n- dbt Core\n- AWS ECS Fargate\n- AWS S3, Glue, Athena\n\n## Python Environment\nUsing uv for fast, reliable Python package management.\nAll projects use pyproject.toml for dependency management.\nEOF\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#step-10-validation-checklist","title":"Step 10: Validation Checklist","text":"<p>Run these commands to verify everything is set up correctly:</p> <pre><code># Check AWS access\naws s3 ls  # Should list your S3 buckets\n\n# Check Pulumi\npulumi whoami  # Should show your identity\n\n# Check Docker\ndocker run hello-world  # Should print success message\n\n# Check Python with uv\nuv run python -c \"import sys; print(sys.version)\"  # Should show 3.12.x\n\n# Check Git\ngit config --global user.name  # Should show your name\ngit config --global user.email  # Should show your email\n\n# Check repositories\ncd ~/workspace/finks\nls -la  # Should show all 4 repositories\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#success-criteria","title":"\u2705 Success Criteria","text":"<p>You're ready to proceed to Week 1 when:</p> <ul> <li>[ ] All tools are installed and working</li> <li>[ ] AWS credentials are configured and tested</li> <li>[ ] Pulumi backend is configured</li> <li>[ ] All 4 GitHub repositories are created and cloned</li> <li>[ ] Docker is running with adequate resources</li> <li>[ ] uv is installed and Python 3.12 environment is set up</li> <li>[ ] All validation commands pass</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#common-issues-and-solutions","title":"\ud83d\udea8 Common Issues and Solutions","text":""},{"location":"roadmaps/agentic-sessions/week-0-preparation/#issue-aws-credentials-not-working","title":"Issue: AWS credentials not working","text":"<pre><code># Check credentials\naws configure list\n\n# Test with a simple command\naws sts get-caller-identity\n\n# If using SSO\naws sso login\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#issue-docker-not-starting","title":"Issue: Docker not starting","text":"<pre><code># macOS: Reset Docker Desktop\nkillall Docker &amp;&amp; open -a Docker\n\n# Check Docker daemon\ndocker version\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#issue-pulumi-state-bucket-access-denied","title":"Issue: Pulumi state bucket access denied","text":"<pre><code># Check bucket exists\naws s3 ls s3://finks-pulumi-state-YOUR_ACCOUNT_ID\n\n# Check bucket policy\naws s3api get-bucket-policy --bucket finks-pulumi-state-YOUR_ACCOUNT_ID\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#next-steps","title":"\ud83d\udccb Next Steps","text":"<p>Once everything is validated: 1. Review the Week 1 Foundation sessions 2. Prepare for Session 1 (Local Development Environment) 3. Ensure Docker is running before starting Session 1 4. Have this checklist handy for reference</p>"},{"location":"roadmaps/agentic-sessions/week-0-preparation/#backup-your-setup","title":"\ud83d\udcbe Backup Your Setup","text":"<p>Before proceeding, backup your configuration:</p> <pre><code># Create backup of configurations\ncd ~/workspace/finks\ntar -czf ../finks-setup-backup.tar.gz .env CONTEXT.md\n\n# Store securely (not in git!)\n</code></pre> <p>Ready to start? Proceed to Week 1: Foundation Sessions</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/","title":"Week 1: Foundation (4 Parallel Sessions)","text":"<p>This week focuses on establishing the core development environment and infrastructure foundation. Sessions 1 and 2 can be run in parallel, followed by Session 3, with Session 4 possible at any time after Session 1.</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#session-overview","title":"Session Overview","text":"Session Duration Dependencies Repository Focus 1 3-4 hours Session 0 finks-infrastructure Local development environment 2 4-5 hours Session 0 finks-infrastructure Pulumi project structure 3 5-6 hours Session 2 finks-infrastructure Core AWS components 4 2-3 hours Session 1 All repos Repository scaffolding"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#session-1-local-development-environment","title":"\ud83d\udfe1 Session 1: Local Development Environment","text":"<p>Repository: <code>finks-infrastructure</code> Estimated Duration: 3-4 hours (includes testing) Dependencies: Session 0 complete Difficulty: \u2b50\u2b50</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#context-prime-for-claude","title":"Context Prime for Claude","text":"<pre><code>I need you to create a complete local development environment for the Finks data pipeline.\nWe're using Docker Compose to simulate AWS services locally with LocalStack.\nFocus on developer experience - it should be easy to start/stop/reset the environment.\nInclude comprehensive error handling and helpful error messages.\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#required-resources","title":"Required Resources","text":"<ul> <li>Docker Desktop running</li> <li>8GB RAM available</li> <li>Ports 4200 (Prefect), 4566 (LocalStack) free</li> <li>uv installed for Python environment management</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#specific-tasks","title":"Specific Tasks","text":"<pre><code>1. Docker Compose Setup (45 min)\n   - Prefect Server with PostgreSQL\n   - LocalStack with S3, Glue, Athena services\n   - Proper health checks\n   - Volume persistence\n\n2. Developer Tools (30 min)\n   - Makefile with targets:\n     - make start\n     - make stop  \n     - make reset\n     - make test\n   - .env.example file\n   - config/ directory with .env.dev template\n\n3. Environment Configuration (15 min)\n   - Set up config/ directory structure\n   - Create .env.dev, .env.staging templates\n   - Add environment loader (see [Environment Management](../../environment-management/index.md))\n\n4. Testing Harness (30 min)\n   - Test script that validates all services\n   - Sample data upload to LocalStack S3\n   - Prefect connection test\n\n5. Documentation (30 min)\n   - README with troubleshooting guide\n   - Architecture diagram\n   - Common issues section\n\n6. Integration Test (30 min)\n   - Full stack startup\n   - Create test Prefect flow\n   - Upload to S3 and verify\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#success-criteria","title":"Success Criteria","text":"<ul> <li>[ ] <code>make start</code> brings up all services in &lt;2 minutes</li> <li>[ ] <code>make test</code> passes all checks</li> <li>[ ] Can create and run a Prefect flow locally</li> <li>[ ] LocalStack S3 operations work</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#common-pitfalls","title":"Common Pitfalls","text":"<ul> <li>Docker resource limits too low</li> <li>Port conflicts with existing services</li> <li>LocalStack Pro features attempted (use free tier only)</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#recovery-strategy","title":"Recovery Strategy","text":"<p>If session fails, commit working Docker Compose and continue in next session</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#template-to-accelerate","title":"Template to Accelerate","text":"<pre><code># docker-compose.yml starter\nversion: '3.8'\nservices:\n  postgres:\n    image: postgres:14-alpine\n    environment:\n      POSTGRES_USER: prefect\n      POSTGRES_PASSWORD: prefect\n      POSTGRES_DB: prefect\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U prefect\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  prefect-server:\n    image: prefecthq/prefect:2-python3.12\n    command: prefect server start\n    environment:\n      PREFECT_SERVER_API_HOST: 0.0.0.0\n      PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@postgres:5432/prefect\n    ports:\n      - \"4200:4200\"\n    depends_on:\n      postgres:\n        condition: service_healthy\n\n  localstack:\n    image: localstack/localstack:latest\n    environment:\n      SERVICES: s3,glue,athena,iam,sts\n      DEFAULT_REGION: ca-central-1\n      DATA_DIR: /tmp/localstack/data\n    ports:\n      - \"4566:4566\"\n    volumes:\n      - \"./localstack:/tmp/localstack\"\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#handoff-documentation","title":"Handoff Documentation","text":"<pre><code>## Session 1 Handoff\n- \u2705 Docker Compose working with all services\n- \u2705 Makefile with developer commands  \n- \u2705 LocalStack S3 tested and working\n- \u2705 Prefect Server accessible at http://localhost:4200\n- \ud83d\udcc1 Files created:\n  - docker-compose.yml\n  - Makefile\n  - .env.example\n  - tests/validate_stack.py\n  - README.md\n- \ud83d\udd17 Next: Can proceed with Session 2 or 4\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#session-2-pulumi-project-structure","title":"\ud83d\udfe1 Session 2: Pulumi Project Structure","text":"<p>Repository: <code>finks-infrastructure</code> Estimated Duration: 4-5 hours (includes testing) Dependencies: Session 0 complete Difficulty: \u2b50\u2b50\u2b50</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#context-prime-for-claude_1","title":"Context Prime for Claude","text":"<pre><code>I need you to create a well-structured Pulumi project for AWS infrastructure.\nUse Python with strong typing and component-based architecture.\nEach component should be testable in isolation.\nFollow Pulumi best practices for resource naming and organization.\nThe goal is a maintainable, extensible infrastructure codebase.\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#required-resources_1","title":"Required Resources","text":"<ul> <li>Pulumi CLI installed and logged in</li> <li>uv installed with Python 3.12+</li> <li>AWS credentials configured</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#specific-tasks_1","title":"Specific Tasks","text":"<pre><code>1. Project Initialization (30 min)\n   - Pulumi new aws-python\n   - Configure pyproject.toml with uv\n   - Add dependencies (pulumi, pulumi-aws, mypy)\n   - Set up type hints and linting\n\n2. Component Architecture (90 min)\n   - BaseComponent abstract class\n   - NetworkingComponent (VPC, subnets)\n   - StorageComponent (S3 buckets)\n   - ComputeComponent (ECS, Lambda)\n   - MonitoringComponent (CloudWatch)\n\n3. Configuration System (45 min)\n   - Environment configs (dev, staging, prod)\n   - Config validation\n   - Secret handling patterns\n\n4. Testing Framework (60 min)\n   - Unit test setup with mocks\n   - Component integration tests\n   - Pulumi testing utilities\n\n5. CI/CD Setup (45 min)\n   - GitHub Actions workflow\n   - Pulumi preview on PR\n   - ESC integration for secrets\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#success-criteria_1","title":"Success Criteria","text":"<ul> <li>[ ] <code>pulumi preview</code> runs without errors</li> <li>[ ] All components have unit tests</li> <li>[ ] Type checking passes with mypy</li> <li>[ ] CI workflow triggers on PR</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#common-pitfalls_1","title":"Common Pitfalls","text":"<ul> <li>Circular dependencies between components</li> <li>Missing resource provider registrations</li> <li>Incorrect typing for Pulumi outputs</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#template-to-accelerate_1","title":"Template to Accelerate","text":"<pre><code># pyproject.toml for finks-infrastructure\n[project]\nname = \"finks-infrastructure\"\nversion = \"0.1.0\"\ndescription = \"Pulumi IaC for Finks data pipeline\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"pulumi&gt;=3.100.0\",\n    \"pulumi-aws&gt;=6.0.0\",\n    \"pulumi-docker&gt;=4.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"mypy&gt;=1.0.0\",\n    \"pytest&gt;=7.0.0\",\n    \"pytest-cov&gt;=4.0.0\",\n    \"ruff&gt;=0.1.0\",\n    \"black&gt;=23.0.0\",\n]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py312\"\n\n[tool.mypy]\npython_version = \"3.12\"\nstrict = true\n</code></pre> <pre><code># components/base.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict\nimport pulumi\n\nclass BaseComponent(pulumi.ComponentResource, ABC):\n    def __init__(self, name: str, args: Dict[str, Any], opts: pulumi.ResourceOptions = None):\n        super().__init__(self._get_type(), name, {}, opts)\n        self.name = name\n        self.args = args\n        self._create_resources()\n        self.register_outputs(self._get_outputs())\n\n    @abstractmethod\n    def _get_type(self) -&gt; str:\n        \"\"\"Return the component type string.\"\"\"\n        pass\n\n    @abstractmethod\n    def _create_resources(self) -&gt; None:\n        \"\"\"Create all resources for this component.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_outputs(self) -&gt; Dict[str, Any]:\n        \"\"\"Return component outputs.\"\"\"\n        pass\n\n# components/networking.py\nfrom typing import Dict, Any\nimport pulumi_aws as aws\nfrom .base import BaseComponent\n\nclass NetworkingComponent(BaseComponent):\n    def _get_type(self) -&gt; str:\n        return \"custom:infrastructure:Networking\"\n\n    def _create_resources(self) -&gt; None:\n        # Create VPC\n        self.vpc = aws.ec2.Vpc(\n            f\"{self.name}-vpc\",\n            cidr_block=\"10.0.0.0/16\",\n            enable_dns_hostnames=True,\n            enable_dns_support=True,\n            tags={\"Name\": f\"{self.name}-vpc\"}\n        )\n\n        # Create subnets...\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#handoff-documentation_1","title":"Handoff Documentation","text":"<pre><code>## Session 2 Handoff\n- \u2705 Pulumi project structure created\n- \u2705 Component-based architecture implemented\n- \u2705 All components have unit tests\n- \u2705 GitHub Actions CI configured\n- \ud83d\udcc1 Files created:\n  - __main__.py\n  - components/base.py\n  - components/networking.py\n  - components/storage.py\n  - components/compute.py\n  - tests/test_components.py\n  - .github/workflows/pulumi.yml\n- \ud83d\udd17 Next: Session 3 can use these components\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#session-3-core-aws-components","title":"\ud83d\udfe1 Session 3: Core AWS Components","text":"<p>Repository: <code>finks-infrastructure</code> Estimated Duration: 5-6 hours (includes testing and deployment) Dependencies: Session 2 complete Difficulty: \u2b50\u2b50\u2b50</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#context-prime-for-claude_2","title":"Context Prime for Claude","text":"<pre><code>I need you to implement the core AWS infrastructure using the Pulumi components from Session 2.\nFocus on VPC with proper networking, S3 buckets with lifecycle policies, and IAM roles.\nDeploy to the dev environment and validate everything works.\nThis forms the foundation for all other services.\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#required-resources_2","title":"Required Resources","text":"<ul> <li>Pulumi components from Session 2</li> <li>AWS account with appropriate permissions</li> <li>Dev environment configuration</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#specific-tasks_2","title":"Specific Tasks","text":"<pre><code>1. VPC Implementation (90 min)\n   - VPC with 10.0.0.0/16 CIDR\n   - 2 public subnets (different AZs)\n   - 2 private subnets (different AZs)\n   - Internet Gateway\n   - NAT Gateway (1 for dev, 2 for prod)\n   - Route tables and associations\n\n2. S3 Buckets (60 min)\n   - Bronze bucket with raw data settings\n   - Silver bucket with lifecycle policies\n   - Gold bucket optimized for queries\n   - Logging bucket for audit trails\n   - Proper bucket policies and CORS\n\n3. Secrets Manager (30 min)\n   - Create secret for FMP API key\n   - Create secret for database credentials\n   - Set up rotation policies\n   - IAM policies for access\n\n4. IAM Roles (60 min)\n   - ECS task execution role\n   - ECS task role (for apps)\n   - Lambda execution role\n   - Glue job role\n   - Cross-service trust policies\n\n5. Deployment &amp; Validation (60 min)\n   - Deploy to dev environment\n   - Validate all resources created\n   - Test S3 access\n   - Document outputs\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#success-criteria_2","title":"Success Criteria","text":"<ul> <li>[ ] VPC with working public/private subnets</li> <li>[ ] All S3 buckets created with correct policies</li> <li>[ ] Secrets Manager storing credentials</li> <li>[ ] IAM roles with least privilege</li> <li>[ ] Successfully deployed to AWS dev</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#common-pitfalls_2","title":"Common Pitfalls","text":"<ul> <li>NAT Gateway in wrong subnet (must be public)</li> <li>S3 bucket names not globally unique</li> <li>IAM policies too permissive</li> <li>Missing resource tags</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#validation-commands","title":"Validation Commands","text":"<pre><code># Verify VPC\naws ec2 describe-vpcs --filters \"Name=tag:Name,Values=finks-dev-vpc\"\n\n# Check subnets\naws ec2 describe-subnets --filters \"Name=vpc-id,Values=&lt;vpc-id&gt;\"\n\n# List S3 buckets\naws s3 ls | grep finks\n\n# Verify secrets\naws secretsmanager list-secrets | grep finks\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#handoff-documentation_2","title":"Handoff Documentation","text":"<pre><code>## Session 3 Handoff\n- \u2705 VPC deployed with proper networking\n- \u2705 S3 buckets created (bronze, silver, gold, logs)\n- \u2705 Secrets Manager configured\n- \u2705 IAM roles created\n- \ud83d\udcca Key Outputs:\n  - VPC ID: vpc-xxxxxxxxx\n  - Bronze Bucket: finks-dev-bronze-xxxxx\n  - Silver Bucket: finks-dev-silver-xxxxx\n  - Gold Bucket: finks-dev-gold-xxxxx\n- \ud83d\udd17 Next: Session 5 (ECS) and 6 (RDS) can proceed\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#session-4-repository-scaffolding","title":"\ud83d\udfe1 Session 4: Repository Scaffolding","text":"<p>Repository: All repositories Estimated Duration: 2-3 hours Dependencies: Session 1 complete Difficulty: \u2b50\u2b50</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#context-prime-for-claude_3","title":"Context Prime for Claude","text":"<pre><code>I need you to set up the initial structure for our three application repositories:\n- finks-pipelines (Prefect flows)\n- finks-dbt (transformation models)\n- finks-ingestion (data connectors)\nEach should have proper Python project structure, testing setup, and Docker configuration.\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#specific-tasks_3","title":"Specific Tasks","text":"<pre><code>1. finks-pipelines Setup (45 min)\n   - Prefect project structure\n   - flows/ directory organization\n   - tests/ with pytest setup\n   - Docker configuration\n   - pyproject.toml with uv\n   - Basic flow template\n\n2. finks-dbt Setup (45 min)\n   - dbt project init\n   - Medallion architecture folders\n   - Docker setup for local dev\n   - profiles.yml template\n   - Basic model examples\n   - Testing structure\n\n3. finks-ingestion Setup (45 min)\n   - Connector architecture\n   - Shared utilities module\n   - Docker template for connectors\n   - Testing framework\n   - Error handling patterns\n   - S3 upload utilities\n\n4. CI/CD Templates (30 min)\n   - GitHub Actions for each repo\n   - Testing workflows\n   - Docker build workflows\n   - Linting and formatting\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#repository-structure-templates","title":"Repository Structure Templates","text":""},{"location":"roadmaps/agentic-sessions/week-1-foundation/#finks-pipelines","title":"finks-pipelines","text":"<pre><code>finks-pipelines/\n\u251c\u2500\u2500 flows/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 ingestion/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 base_ingestion.py\n\u2502   \u251c\u2500\u2500 transformation/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 base_transformation.py\n\u2502   \u2514\u2500\u2500 master/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 daily_pipeline.py\n\u251c\u2500\u2500 tasks/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 common.py\n\u251c\u2500\u2500 blocks/\n\u2502   \u2514\u2500\u2500 configure_blocks.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_flows.py\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 .python-version\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#finks-dbt","title":"finks-dbt","text":"<pre><code>finks-dbt/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u251c\u2500\u2500 fmp/\n\u2502   \u2502   \u2514\u2500\u2500 _sources.yml\n\u2502   \u251c\u2500\u2500 intermediate/\n\u2502   \u2514\u2500\u2500 marts/\n\u2502       \u251c\u2500\u2500 finance/\n\u2502       \u2514\u2500\u2500 _models.yml\n\u251c\u2500\u2500 tests/\n\u251c\u2500\u2500 macros/\n\u251c\u2500\u2500 seeds/\n\u251c\u2500\u2500 dbt_project.yml\n\u251c\u2500\u2500 profiles.yml\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#finks-ingestion","title":"finks-ingestion","text":"<pre><code>finks-ingestion/\n\u251c\u2500\u2500 connectors/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2514\u2500\u2500 fmp/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 connector.py\n\u2502       \u2514\u2500\u2500 schemas.py\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 s3.py\n\u2502   \u2514\u2500\u2500 validation.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_connectors.py\n\u251c\u2500\u2500 Dockerfile.template\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#success-criteria_3","title":"Success Criteria","text":"<ul> <li>[ ] All repos have consistent structure</li> <li>[ ] Python projects configured with uv and pyproject.toml</li> <li>[ ] Docker files ready for use</li> <li>[ ] CI/CD workflows in place</li> <li>[ ] READMEs with setup instructions</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#pyprojecttoml-templates","title":"pyproject.toml Templates","text":""},{"location":"roadmaps/agentic-sessions/week-1-foundation/#finks-pipelines_1","title":"finks-pipelines","text":"<pre><code>[project]\nname = \"finks-pipelines\"\nversion = \"0.1.0\"\ndescription = \"Prefect orchestration flows for Finks data pipeline\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"prefect&gt;=2.14.0\",\n    \"prefect-aws&gt;=0.4.0\",\n    \"boto3&gt;=1.28.0\",\n    \"pydantic&gt;=2.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=7.0.0\",\n    \"pytest-asyncio&gt;=0.21.0\",\n    \"ruff&gt;=0.1.0\",\n    \"black&gt;=23.0.0\",\n]\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#finks-dbt_1","title":"finks-dbt","text":"<pre><code>[project]\nname = \"finks-dbt\"\nversion = \"0.1.0\"\ndescription = \"dbt transformation models for Finks data pipeline\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"dbt-core&gt;=1.7.0\",\n    \"dbt-athena-community&gt;=1.7.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=7.0.0\",\n    \"ruff&gt;=0.1.0\",\n    \"sqlfluff&gt;=3.0.0\",\n]\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#finks-ingestion_1","title":"finks-ingestion","text":"<pre><code>[project]\nname = \"finks-ingestion\"\nversion = \"0.1.0\"\ndescription = \"Data source connectors for Finks pipeline\"\nrequires-python = \"&gt;=3.12\"\ndependencies = [\n    \"boto3&gt;=1.28.0\",\n    \"requests&gt;=2.31.0\",\n    \"pydantic&gt;=2.0.0\",\n    \"pandas&gt;=2.0.0\",\n    \"pyarrow&gt;=14.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=7.0.0\",\n    \"pytest-mock&gt;=3.0.0\",\n    \"ruff&gt;=0.1.0\",\n    \"black&gt;=23.0.0\",\n]\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#handoff-documentation_3","title":"Handoff Documentation","text":"<pre><code>## Session 4 Handoff\n- \u2705 finks-pipelines repository structured\n- \u2705 finks-dbt repository initialized\n- \u2705 finks-ingestion repository ready\n- \u2705 All repos have CI/CD workflows\n- \ud83d\udcc1 Ready for development:\n  - Prefect flows can be created\n  - dbt models can be added\n  - Connectors can be implemented\n- \ud83d\udd17 Next: Any Week 2 session can proceed\n</code></pre>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#week-1-validation-gate","title":"Week 1 Validation Gate","text":"<p>Before proceeding to Week 2, ensure:</p>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#infrastructure","title":"Infrastructure","text":"<ul> <li>[ ] Local development environment fully functional</li> <li>[ ] Pulumi project structured and tested</li> <li>[ ] Core AWS resources deployed to dev</li> <li>[ ] All repositories initialized</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#testing","title":"Testing","text":"<ul> <li>[ ] Can run Prefect flows locally</li> <li>[ ] Can deploy infrastructure changes</li> <li>[ ] All unit tests passing</li> <li>[ ] CI/CD pipelines working</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#documentation","title":"Documentation","text":"<ul> <li>[ ] All handoffs documented</li> <li>[ ] READMEs updated</li> <li>[ ] Issues tracked</li> <li>[ ] Next steps clear</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-1-foundation/#ready-for-week-2","title":"Ready for Week 2?","text":"<p>If all items are checked, proceed to Week 2: Infrastructure Completion</p>"},{"location":"roadmaps/agentic-sessions/week-2-infrastructure/","title":"Week 2: Infrastructure Completion (Sessions 5-7)","text":"<p>This week focuses on completing the core infrastructure components needed for the data pipeline.</p>"},{"location":"roadmaps/agentic-sessions/week-2-infrastructure/#session-overview","title":"Session Overview","text":"Session Duration Dependencies Repository Focus 5 4-5 hours Session 3 finks-infrastructure ECS infrastructure 6 3-4 hours Session 3 finks-infrastructure RDS and networking 7 3-4 hours Session 4 All repos CI/CD pipelines"},{"location":"roadmaps/agentic-sessions/week-2-infrastructure/#content-coming-soon","title":"Content Coming Soon","text":"<p>The detailed session guides for Week 2 are being prepared. Check back soon or refer to the original timeline for session details.</p>"},{"location":"roadmaps/agentic-sessions/week-2-infrastructure/#week-2-objectives","title":"Week 2 Objectives","text":"<ul> <li>Complete ECS cluster setup with Fargate</li> <li>Deploy RDS PostgreSQL for Prefect</li> <li>Finalize networking with security groups</li> <li>Implement CI/CD for all repositories</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-2-infrastructure/#prerequisites","title":"Prerequisites","text":"<ul> <li>Week 1 validation gate passed</li> <li>Core AWS infrastructure deployed</li> <li>All repositories initialized</li> </ul> <p>Back to Overview | Previous: Week 1 | Next: Week 3</p>"},{"location":"roadmaps/agentic-sessions/week-3-orchestration/","title":"Week 3: Orchestration Platform (Sessions 8-11)","text":"<p>This week focuses on deploying and configuring the Prefect orchestration platform.</p>"},{"location":"roadmaps/agentic-sessions/week-3-orchestration/#session-overview","title":"Session Overview","text":"Session Duration Dependencies Repository Focus 8 6-7 hours Sessions 5, 6 finks-infrastructure Prefect server deployment 9 3-4 hours Session 8 finks-infrastructure Prefect agent setup 10 2-3 hours Session 9 finks-pipelines Prefect blocks configuration 11 3-4 hours Session 8 finks-infrastructure Monitoring setup"},{"location":"roadmaps/agentic-sessions/week-3-orchestration/#content-coming-soon","title":"Content Coming Soon","text":"<p>The detailed session guides for Week 3 are being prepared. Check back soon or refer to the original timeline for session details.</p>"},{"location":"roadmaps/agentic-sessions/week-3-orchestration/#week-3-objectives","title":"Week 3 Objectives","text":"<ul> <li>Deploy Prefect Server on ECS Fargate</li> <li>Configure Prefect agents for job execution</li> <li>Set up Prefect blocks as code</li> <li>Implement basic monitoring and alerting</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-3-orchestration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Week 2 validation gate passed</li> <li>ECS cluster operational</li> <li>RDS database deployed</li> <li>ALB configured with SSL</li> </ul> <p>Back to Overview | Previous: Week 2 | Next: Week 4</p>"},{"location":"roadmaps/agentic-sessions/week-4-transformation/","title":"Week 4: Transformation Infrastructure (Sessions 12-14)","text":"<p>This week focuses on setting up the data transformation layer with dbt and AWS Glue.</p>"},{"location":"roadmaps/agentic-sessions/week-4-transformation/#session-overview","title":"Session Overview","text":"Session Duration Dependencies Repository Focus 12 4-5 hours Sessions 5, 8 finks-dbt, finks-infrastructure dbt on ECS 13 3-4 hours Session 3 finks-infrastructure, finks-pipelines Glue setup 14 3-4 hours Session 12 finks-dbt Data models design"},{"location":"roadmaps/agentic-sessions/week-4-transformation/#content-coming-soon","title":"Content Coming Soon","text":"<p>The detailed session guides for Week 4 are being prepared. Check back soon or refer to the original timeline for session details.</p>"},{"location":"roadmaps/agentic-sessions/week-4-transformation/#week-4-objectives","title":"Week 4 Objectives","text":"<ul> <li>Containerize dbt for ECS execution</li> <li>Configure AWS Glue for Bronze\u2192Silver processing</li> <li>Design and implement core dbt models</li> <li>Set up data quality tests</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-4-transformation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Week 3 validation gate passed</li> <li>Prefect operational</li> <li>S3 buckets configured</li> <li>Athena available</li> </ul> <p>Back to Overview | Previous: Week 3 | Next: Week 5</p>"},{"location":"roadmaps/agentic-sessions/week-5-pipeline/","title":"Week 5: End-to-End Pipeline (Sessions 15-18)","text":"<p>This week focuses on building the complete data pipeline from ingestion to transformation.</p>"},{"location":"roadmaps/agentic-sessions/week-5-pipeline/#session-overview","title":"Session Overview","text":"Session Duration Dependencies Repository Focus 15 4-5 hours Session 4 finks-ingestion FMP connector 16 7-8 hours Sessions 10, 14, 15 finks-pipelines Orchestration flows 17 3-4 hours Session 16 finks-dbt, finks-pipelines Data quality 18 3-4 hours Session 16 finks-pipelines Historical backfill"},{"location":"roadmaps/agentic-sessions/week-5-pipeline/#content-coming-soon","title":"Content Coming Soon","text":"<p>The detailed session guides for Week 5 are being prepared. Check back soon or refer to the original timeline for session details.</p>"},{"location":"roadmaps/agentic-sessions/week-5-pipeline/#week-5-objectives","title":"Week 5 Objectives","text":"<ul> <li>Build FMP API connector with error handling</li> <li>Create Prefect flows for orchestration</li> <li>Implement data quality checks</li> <li>Enable historical data backfilling</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-5-pipeline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Week 4 validation gate passed</li> <li>Transformation infrastructure ready</li> <li>Prefect blocks configured</li> <li>dbt models created</li> </ul> <p>Back to Overview | Previous: Week 4 | Next: Week 6</p>"},{"location":"roadmaps/agentic-sessions/week-6-production/","title":"Week 6: Production Readiness (Sessions 19-22)","text":"<p>This final week focuses on making the pipeline production-ready with APIs, monitoring, and security.</p>"},{"location":"roadmaps/agentic-sessions/week-6-production/#session-overview","title":"Session Overview","text":"Session Duration Dependencies Repository Focus 19 4-5 hours Session 16 finks-infrastructure API development 20 3-4 hours Session 11 finks-infrastructure Advanced monitoring 21 4-5 hours Session 19 All repos Security hardening 22 3-4 hours Session 21 All repos Documentation"},{"location":"roadmaps/agentic-sessions/week-6-production/#content-coming-soon","title":"Content Coming Soon","text":"<p>The detailed session guides for Week 6 are being prepared. Check back soon or refer to the original timeline for session details.</p>"},{"location":"roadmaps/agentic-sessions/week-6-production/#week-6-objectives","title":"Week 6 Objectives","text":"<ul> <li>Build API Gateway with Lambda for data access</li> <li>Implement comprehensive monitoring and alerting</li> <li>Harden security across all components</li> <li>Create operational documentation</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-6-production/#prerequisites","title":"Prerequisites","text":"<ul> <li>Week 5 validation gate passed</li> <li>End-to-end pipeline working</li> <li>Data flowing to Gold zone</li> <li>Basic monitoring in place</li> </ul>"},{"location":"roadmaps/agentic-sessions/week-6-production/#final-deliverables","title":"Final Deliverables","text":"<ul> <li>Production-ready data pipeline</li> <li>API for data access</li> <li>Comprehensive monitoring</li> <li>Security audit complete</li> <li>Full documentation</li> <li>Operational runbooks</li> </ul> <p>Back to Overview | Previous: Week 5 | Back to Start</p>"},{"location":"security-architecture/","title":"Security","text":""},{"location":"security-architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"security-architecture/#network-security","title":"Network Security","text":"<pre><code>graph TD;\n    subgraph \"VPC (10.0.0.0/16)\";\n        subgraph \"Public Subnet\";\n            ALB;\n            NAT_Gateway[\"NAT Gateway\"];\n        end;\n        subgraph \"Private Subnet\";\n            ECS_Tasks[\"ECS Tasks, RDS, ElastiCache\"];\n            VPC_Endpoints[\"VPC Endpoints (S3, DynamoDB)\"];\n        end;\n    end;\n\n    ALB --&gt; ECS_Tasks;\n    ECS_Tasks -- \"Egress\" --&gt; NAT_Gateway;\n    ECS_Tasks -- \"AWS Services\" --&gt; VPC_Endpoints;</code></pre>"},{"location":"security-architecture/#identity-access-management","title":"Identity &amp; Access Management","text":"<ul> <li>Service Accounts: Each service has minimal IAM role</li> <li>Cross-Account Roles: For multi-account deployments</li> <li>Temporary Credentials: No long-lived access keys</li> <li>MFA Enforcement: For human users</li> </ul>"},{"location":"security-architecture/#data-security","title":"Data Security","text":"<ul> <li>Encryption at Rest: S3 SSE-S3, RDS encryption</li> <li>Encryption in Transit: TLS 1.2+ everywhere</li> <li>Key Management: AWS KMS with key rotation</li> <li>Data Masking: PII masked in Silver/Gold zones</li> </ul>"},{"location":"security-architecture/#secrets-management","title":"Secrets Management","text":"<pre><code>graph TD\n    subgraph AWS Secrets Manager\n        A[\"/prefect/database/connection\"];\n        B[\"/mongodb/connection-string\"];\n        C[\"/fmp-api/api-key\"];\n        D[\"/dbt/profiles/production\"];\n        E[\"/monitoring/datadog-api-key\"];\n    end</code></pre>"},{"location":"service-architecture/","title":"Service Architecture","text":""},{"location":"service-architecture/#service-architecture","title":"Service Architecture","text":""},{"location":"service-architecture/#prefect-server-deployment","title":"Prefect Server Deployment","text":"<ul> <li>Infrastructure: ECS Fargate with auto-scaling</li> <li>Database: RDS PostgreSQL Multi-AZ</li> <li>Load Balancer: ALB with SSL termination</li> <li>Storage: EFS for shared artifacts</li> </ul>"},{"location":"service-architecture/#prefect-agent-architecture","title":"Prefect Agent Architecture","text":"<pre><code>graph TD\n    subgraph Agent Pools\n        A[\"ingestion-pool&lt;br/&gt;# High memory for data extraction\"];\n        B[\"transformation-pool&lt;br/&gt;# High CPU for dbt runs\"];\n        C[\"lightweight-pool&lt;br/&gt;# Burstable for notifications\"];\n        D[\"gpu-pool&lt;br/&gt;# ML workloads (future)\"];\n    end</code></pre>"},{"location":"service-architecture/#ecs-task-patterns","title":"ECS Task Patterns","text":""},{"location":"service-architecture/#ingestion-tasks-ephemeral","title":"Ingestion Tasks (Ephemeral)","text":"<ul> <li>Lifecycle: Created by Prefect, terminated after completion</li> <li>Resources: Configured per data source requirements</li> <li>Networking: Private subnet with NAT Gateway</li> <li>IAM Role: Scoped to specific S3 paths and secrets</li> </ul>"},{"location":"service-architecture/#dbt-core-on-ecs-ephemeral-tasks","title":"dbt Core on ECS (Ephemeral Tasks)","text":"<ul> <li>Deployment: Ephemeral ECS tasks triggered by Prefect (not long-running service)</li> <li>Architecture:   <pre><code>graph TD\n  subgraph ECS Task Definition\n      A[\"dbt Container\"] --&gt; B[\"dbt project pulled from Git at runtime\"];\n      A --&gt; C[\"Profiles from Secrets Manager\"];\n      A --&gt; D[\"Connection to Athena/Redshift\"];\n      E[\"Sidecar Container (Optional)\"] --&gt; F[\"Metrics collector\"];\n  end</code></pre></li> </ul> <pre><code>- **Execution Model**: Each dbt run is a fresh ECS task\n- **Scaling**: Unlimited parallel tasks based on workload\n- **Storage**: S3 for dbt artifacts and logs\n</code></pre>"},{"location":"technology-decisions/","title":"Technology Decisions","text":""},{"location":"technology-decisions/#technology-decisions","title":"Technology Decisions","text":""},{"location":"technology-decisions/#why-pulumi-over-terraform","title":"Why Pulumi over Terraform?","text":"<ul> <li>Type Safety: Catch errors before deployment</li> <li>Programming Constructs: Loops, conditionals, functions</li> <li>IDE Support: Full IntelliSense and refactoring</li> <li>Testing: Unit test infrastructure code</li> <li>Abstraction: Create high-level components</li> </ul>"},{"location":"technology-decisions/#why-dbt-core-on-ecs-over-alternatives","title":"Why dbt-core on ECS over alternatives?","text":"<ul> <li>Control: Full control over dbt version and plugins</li> <li>Cost: No per-seat licensing like dbt Cloud</li> <li>Integration: Direct integration with Prefect</li> <li>Customization: Add custom packages and macros</li> <li>Security: Runs in your VPC with your IAM roles</li> </ul>"},{"location":"technology-decisions/#why-medallion-architecture","title":"Why Medallion Architecture?","text":"<ul> <li>Clear Quality Tiers: Each zone has defined quality</li> <li>Reprocessing: Can rebuild Silver/Gold from Bronze</li> <li>Schema Evolution: Raw data preserved for new requirements</li> <li>Audit Trail: Complete lineage from source to insight</li> </ul>"},{"location":"technology-decisions/#why-ephemeral-ecs-tasks","title":"Why Ephemeral ECS Tasks?","text":"<ul> <li>Cost Optimization: Pay only for actual runtime</li> <li>Resource Efficiency: No idle containers</li> <li>Isolation: Each run starts fresh</li> <li>Scalability: Unlimited parallel tasks</li> </ul>"},{"location":"technology-decisions/#developer-experience-local-development","title":"Developer Experience &amp; Local Development","text":""},{"location":"technology-decisions/#local-development-stack","title":"Local Development Stack","text":"<pre><code># docker-compose.yml for local development\nversion: \"3.8\"\nservices:\n  prefect-server:\n    image: prefecthq/prefect:latest\n    command: prefect server start\n    environment:\n      PREFECT_SERVER_API_HOST: 0.0.0.0\n    ports:\n      - \"4200:4200\"\n\n  postgres:\n    image: postgres:latest\n    environment:\n      POSTGRES_DB: prefect\n      POSTGRES_USER: prefect\n      POSTGRES_PASSWORD: prefect\n\n  localstack:\n    image: localstack/localstack:latest\n    environment:\n      SERVICES: s3,glue,athena,secretsmanager\n      DEFAULT_REGION: ca-central-1\n    ports:\n      - \"4566:4566\"\n\n  dbt:\n    build: ./finks-dbt\n    volumes:\n      - ./finks-dbt:/dbt\n    environment:\n      DBT_PROFILES_DIR: /dbt/profiles\n</code></pre>"},{"location":"technology-decisions/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Infrastructure Development:</p> </li> <li> <p>Use Pulumi with local state for experimentation</p> </li> <li>Test components against LocalStack</li> <li> <p>Validate IAM policies with AWS IAM Policy Simulator</p> </li> <li> <p>Pipeline Development:</p> </li> <li> <p>Develop Prefect flows locally against LocalStack S3</p> </li> <li>Test dbt models with subset of production data</li> <li> <p>Use dbt's <code>--defer</code> flag to reference production models</p> </li> <li> <p>Testing Strategy:</p> </li> <li>Unit tests for Pulumi components</li> <li>Integration tests for Prefect flows</li> <li>dbt data tests for transformations</li> <li>End-to-end tests in staging environment</li> </ol>"},{"location":"technology-stack/","title":"Technology Stack","text":""},{"location":"technology-stack/#technology-stack-overview","title":"Technology Stack Overview","text":""},{"location":"technology-stack/#core-technologies","title":"Core Technologies","text":"<ul> <li>Pulumi: Infrastructure as Code platform using real programming languages to define and deploy all AWS resources</li> <li>Prefect: Workflow orchestration platform that schedules, monitors, and manages all data pipeline tasks</li> <li>AWS ECS (Fargate): Serverless container platform hosting Prefect components, data connectors, and dbt processes</li> <li>dbt-core: SQL-based transformation framework running on ECS for turning raw data into analytics-ready datasets</li> <li>AWS S3: Object storage serving as the data lake with Bronze/Silver/Gold zones</li> <li>AWS Glue: Managed ETL service for Bronze-to-Silver data cleansing and schema validation</li> <li>AWS Athena: Serverless SQL query engine for analyzing data directly in S3</li> <li>Kinesis Data Streams: Real-time data ingestion for streaming sources</li> <li>Kinesis Firehose: Managed service for reliably loading streaming data into S3</li> <li>AWS Lambda: Serverless functions for event processing and lightweight API endpoints</li> <li>PostgreSQL RDS: Managed database storing Prefect metadata and orchestration state</li> <li>DynamoDB: NoSQL database for tracking data lineage and pipeline metadata</li> <li>AWS Secrets Manager: Secure storage for API keys, database credentials, and sensitive configuration</li> <li>API Gateway: Managed API service for exposing data lake contents to consumers</li> <li>CloudWatch: Monitoring service for logs, metrics, and operational insights</li> <li>Docker: Container technology for packaging all services and ensuring consistency across environments</li> <li>GitHub Actions: CI/CD platform for automated testing and deployment of infrastructure and code changes</li> </ul>"},{"location":"transformation-layer/","title":"Transformation with dbt","text":""},{"location":"transformation-layer/#transformation-layer-with-dbt","title":"Transformation Layer with dbt","text":""},{"location":"transformation-layer/#dbt-on-ecs-architecture","title":"dbt on ECS Architecture","text":"<p>Running dbt-core on ECS provides:</p> <ul> <li>Isolation: Each dbt run in clean environment</li> <li>Scalability: Multiple concurrent transformations</li> <li>Version Control: Docker images tagged with dbt version</li> <li>Resource Management: CPU/memory limits per project</li> </ul>"},{"location":"transformation-layer/#dbt-project-structure","title":"dbt Project Structure","text":"<pre><code>finks-dbt/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 staging/           # Silver zone queries\n\u2502   \u2502   \u251c\u2500\u2500 stg_market_data.sql\n\u2502   \u2502   \u2514\u2500\u2500 stg_transactions.sql\n\u2502   \u251c\u2500\u2500 intermediate/      # Complex joins\n\u2502   \u2502   \u2514\u2500\u2500 int_enriched_trades.sql\n\u2502   \u2514\u2500\u2500 marts/            # Gold zone outputs\n\u2502       \u251c\u2500\u2500 finance/\n\u2502       \u2502   \u2514\u2500\u2500 daily_portfolio_summary.sql\n\u2502       \u2514\u2500\u2500 risk/\n\u2502           \u2514\u2500\u2500 var_calculations.sql\n\u251c\u2500\u2500 tests/                # Data quality tests\n\u251c\u2500\u2500 macros/              # Reusable SQL functions\n\u2514\u2500\u2500 docs/                # Business documentation\n</code></pre>"},{"location":"transformation-layer/#dbt-execution-pattern-with-prefect-integration","title":"dbt Execution Pattern with Prefect Integration","text":"<pre><code># Using Prefect's native ECS integration for robust task execution\nfrom prefect import task, flow\nfrom prefect_aws.ecs import ECSTask\n\n@task\ndef run_dbt_models(model_selector: str, target: str):\n    # Load pre-configured ECS task block\n    ecs_task_runner = ECSTask.load(\"dbt-ecs-task\")\n\n    # Run dbt as ephemeral ECS task with proper monitoring\n    return ecs_task_runner.run(\n        task_definition_arn=\"arn:aws:ecs:region:account:task-definition/dbt-core-task\",\n        overrides={\n            \"containerOverrides\": [{\n                \"name\": \"dbt\",\n                \"command\": [\"dbt\", \"run\", \"--select\", model_selector],\n                \"environment\": [\n                    {\"name\": \"DBT_TARGET\", \"value\": target},\n                    {\"name\": \"DBT_PROFILES_DIR\", \"value\": \"/secrets\"}\n                ]\n            }]\n        },\n        wait_for_completion=True,\n        poll_interval=10\n    )\n\n@flow\ndef dbt_transformation_flow(models: list[str], target: str = \"prod\"):\n    # Run models in parallel using Prefect's native concurrency\n    for model in models:\n        run_dbt_models.submit(model, target)\n</code></pre>"},{"location":"transformation-layer/#cicd-workflow-for-dbt-slim-ci-pattern","title":"CI/CD Workflow for dbt (Slim CI Pattern)","text":"<pre><code># .github/workflows/dbt-pr.yml\nname: dbt Pull Request CI\n\non:\n  pull_request:\n    paths:\n      - \"finks-dbt/**\"\n\njobs:\n  slim-ci:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Download production manifest\n        run: |\n          aws s3 cp s3://dbt-artifacts/prod/manifest.json ./prod_manifest.json\n\n      - name: Run dbt Slim CI\n        run: |\n          # Only build and test modified models and their downstream dependencies\n          dbt build --select state:modified+ --defer --state ./prod_manifest\n        env:\n          DBT_TARGET: ci\n          DBT_PROFILES_DIR: ./ci_profiles\n\n      - name: Upload CI artifacts\n        if: always()\n        run: |\n          aws s3 sync ./target s3://dbt-artifacts/ci/${{ github.sha }}/\n</code></pre>"},{"location":"transformation-layer/#artifact-promotion-strategy","title":"Artifact Promotion Strategy","text":"<p>The CI/CD pipeline follows a \"build once, deploy many\" strategy. A Docker image is built and tested in the staging environment. To promote to production, the exact same image is re-tagged (e.g., from <code>myapp:staging-v1.2</code> to <code>myapp:prod-v1.2</code>) and deployed. No new code is introduced during promotion, guaranteeing that what was tested is what is released.</p> <pre><code># Example promotion script\ndocker pull myregistry/dbt-core:staging-v1.2\ndocker tag myregistry/dbt-core:staging-v1.2 myregistry/dbt-core:prod-v1.2\ndocker push myregistry/dbt-core:prod-v1.2\n# Update ECS task definition to use prod-v1.2\n</code></pre>"},{"location":"transformation-layer/#data-quality-integration","title":"Data Quality Integration","text":"<p>The architecture uses a unified approach to data quality:</p> <ul> <li>dbt-expectations: Primary data quality framework integrated directly into dbt</li> <li>dbt tests: Execute as part of transformation flows</li> <li>Quality gates: Failed tests block data promotion to Gold zone</li> </ul> <p>Example dbt-expectations test:</p> <pre><code># models/silver/schema.yml\nmodels:\n  - name: silver_market_data\n    tests:\n      - dbt_expectations.expect_table_row_count_to_be_between:\n          min_value: 1000\n          max_value: 1000000\n    columns:\n      - name: price\n        tests:\n          - not_null\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 0\n              max_value: 1000000\n</code></pre>"}]}